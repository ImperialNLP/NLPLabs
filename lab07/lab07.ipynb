{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "lab08.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "lab",
      "language": "python",
      "name": "lab"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVEmPHTK-7Kd"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "The goal of this tutorial is to implement a Variational Autoencoder (VAE) for Topic Models. The aim is to give you sense of: \n",
        "\n",
        "\n",
        "*   How topic models can be implemented under Variational Autoencoder (VAE)\n",
        "*   How the \"*reparametrization trick*\" enables backpropogation through latent variables\n",
        "\n",
        "\n",
        "Frist, we need to import neccesary packages:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eGKih-y-7Ki",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd2627a8-81a7-4679-d654-02569e7bfe00"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.autograd import Variable\n",
        "from torch.nn import Parameter\n",
        "import torch.nn.functional as F\n",
        "import math\n",
        "import os\n",
        "import string\n",
        "import numpy as np\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from collections import OrderedDict\n",
        "from tqdm.notebook import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApPPC4zTSmkC",
        "outputId": "74ff080f-e1c5-4602-91be-01caf44567fb"
      },
      "source": [
        "###############\n",
        "# Torch setup #\n",
        "###############\n",
        "print('Torch version: {}, CUDA: {}'.format(torch.__version__, torch.version.cuda))\n",
        "cuda_available = torch.cuda.is_available()\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'\n",
        "\n",
        "#########################\n",
        "# Some helper functions #\n",
        "#########################\n",
        "def fix_seed(seed=None):\n",
        "  \"\"\"Sets the seeds of random number generators.\"\"\"\n",
        "  if seed is None:\n",
        "    # Take a random seed\n",
        "    seed = time.time()\n",
        "  seed = int(seed)\n",
        "  np.random.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  return seed\n",
        "\n",
        "fix_seed(1234)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--RQRbdR-7Ki"
      },
      "source": [
        "## Data Preprocessing\n",
        "### Download dataset\n",
        "\n",
        "We experiment on a standard news corpora: the ***20NewsGroups*** and download it using scikit-learn. This dataset consists of 20k news articles classified into 20 topics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kQmS_Zqu-7Kj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c54b1ec-4164-4410-ae2b-2328ba162dd4"
      },
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "train_news_group = fetch_20newsgroups(subset='train')\n",
        "test_news_group = fetch_20newsgroups(subset='test')\n",
        "\n",
        "train_data = train_news_group['data']\n",
        "test_data = test_news_group['data']\n",
        "\n",
        "print(\"Size of training data:\", len(train_data))\n",
        "print(\"Size of test data:\", len(test_data))\n",
        "print(\"All topics:\", train_news_group.target_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SEol3Jgp-7Kj"
      },
      "source": [
        "### Preprocess Dataset\n",
        "\n",
        "In this section, we define the functions to do conventional preprocessing and build the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1XdgwSg-7Kk"
      },
      "source": [
        "def preprocess(samples):\n",
        "    output = []\n",
        "    for item in samples:\n",
        "        words = item.replace('\\n', '').strip().lower().split(' ')\n",
        "        punctuations = (string.punctuation).replace(\"'\", \"\")\n",
        "        trans_table = str.maketrans('', '', punctuations)\n",
        "        stripped_words = [word.translate(trans_table) for word in words]\n",
        "        words = [str for str in stripped_words if str]\n",
        "        words = [word for word in words if not word.isdigit()]\n",
        "        words = [str for str in words if str]\n",
        "        output.append(words)\n",
        "    return output\n",
        "\n",
        "train_prep = preprocess(train_data)\n",
        "test_prep = preprocess(test_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fskcldry-7Kk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "87578a83-2875-4edc-f086-fe188217f4ae"
      },
      "source": [
        "def get_vocab(data):\n",
        "    vocab = {}\n",
        "    \n",
        "    stops = set(stopwords.words('english'))\n",
        "    ### -------------- TODO --------------- ###\n",
        "    # remove stop words and count frequency of words\n",
        "    \n",
        "    return vocab\n",
        "\n",
        "\n",
        "vocab_total = get_vocab(train_prep + test_prep)\n",
        "print(\"Total number of words in vocabulary:\", len(vocab_total))\n",
        "sorted(vocab_total.items(),key = lambda x:x[1],reverse = True)\n",
        "vocab = vocab_total\n",
        "\n",
        "'''\n",
        "Here we filter vocabulary to save some training time, otherwise our model input dimension would be huge (V=350k+).\n",
        "You can uncomment the line to include more words (around 52k words in vocabulary), which would help classifing the topics (Q4).\n",
        "'''\n",
        "vocab = {k:v for k,v in list(vocab_total.items())[:5000]}\n",
        "# vocab = {k:v for k,v in vocab_total.items() if v > 3}\n",
        "vocab_size = len(vocab)\n",
        "print(\"Vocabulary size after filtering:\", vocab_size)\n",
        "\n",
        "word2idx = {k:n for n,(k,v) in enumerate(vocab.items())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R4WHKBGT-7Kk"
      },
      "source": [
        "train_doc = [[word for word in doc if word in vocab] for doc in train_prep]\n",
        "train_doc = [doc for doc in train_doc if len(doc) > 5]\n",
        "\n",
        "test_doc = [[word for word in doc if word in vocab] for doc in test_prep]\n",
        "test_doc = [doc for doc in test_doc if len(doc) > 5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMQRBCmA-7Kk"
      },
      "source": [
        "### Process Bag-of-words Inputs\n",
        "\n",
        "Next we define multiple helper functions to create input batches. Our inputs are represented in bag-of-word (bow) where each article/document is represented with a vector of **V** elements. We will also do the batching in this section, so the inputs to the models would be in the dimension of *(batch_size, vocab_size)*."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZhYi_u_-7Kl"
      },
      "source": [
        "from collections import Counter\n",
        "def data_set(data_url):\n",
        "    \"\"\"process data input.\"\"\"\n",
        "    data = []\n",
        "    word_count = []\n",
        "    for words in data_url:\n",
        "        word2freq = dict(Counter(words))\n",
        "        doc = {}\n",
        "        count = 0\n",
        "\n",
        "        for word,freq in word2freq.items():\n",
        "            doc[int(word2idx[word])] = freq\n",
        "            count += freq\n",
        "\n",
        "        if count > 0:\n",
        "            data.append(doc)\n",
        "            word_count.append(count)\n",
        "\n",
        "    return data, word_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUw2w3jn-7Kl"
      },
      "source": [
        "def create_batches(data_size, batch_size, shuffle=True):\n",
        "    \"\"\"create a batch of indices.\"\"\"\n",
        "    batches = []\n",
        "    ids = list(range(data_size))\n",
        "    if shuffle:\n",
        "        random.shuffle(ids)\n",
        "    for i in range(data_size // batch_size):\n",
        "        start = i * batch_size\n",
        "        end = (i + 1) * batch_size\n",
        "        batches.append(ids[start:end])\n",
        "    # the batch of which the length is less than batch_size\n",
        "    rest = data_size % batch_size\n",
        "    if rest > 0:\n",
        "        batches.append(ids[-rest:] + [-1] * (batch_size - rest))  # -1 as padding\n",
        "    return batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU-GfOj4-7Kl"
      },
      "source": [
        "def fetch_data(data, count, idx_batch, vocab_size):\n",
        "    \"\"\"fetch input data by batch.\"\"\"\n",
        "    batch_size = len(idx_batch)\n",
        "    data_batch = np.zeros((batch_size, vocab_size))\n",
        "    count_batch = []\n",
        "    mask = np.zeros(batch_size)\n",
        "    indices = []\n",
        "    values = []\n",
        "    for i, doc_id in enumerate(idx_batch):\n",
        "        if doc_id != -1:\n",
        "            for word_id, freq in data[doc_id].items():\n",
        "                data_batch[i, word_id] = freq\n",
        "            count_batch.append(count[doc_id])\n",
        "            mask[i]=1.0\n",
        "        else:\n",
        "            count_batch.append(0)\n",
        "    return data_batch, count_batch, mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqyxSRTY-7Kl"
      },
      "source": [
        "### Question 1: Finish the neural structures of the VAE encoder and decoder, and the reparamerisation trick."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COC0bLTL-7Km"
      },
      "source": [
        "class TopicModel(nn.Module):\n",
        "    def __init__(self, \n",
        "                 vocab_size,\n",
        "                 input_size,\n",
        "                 n_hidden,\n",
        "                 n_topic, \n",
        "                 batch_size):\n",
        "        super(TopicModel, self).__init__()\n",
        "\n",
        "        self.vocab_size = vocab_size\n",
        "        self.n_hidden = n_hidden\n",
        "        self.n_topic = n_topic\n",
        "        self.batch_size = batch_size\n",
        " \n",
        "        ### -------------- TODO --------------- ###\n",
        "        self.mu_layer = \n",
        "        self.logsigm_layer = \n",
        "\n",
        "        ### -------------- TODO --------------- ###\n",
        "        self.encoder = nn.Sequential(nn.Linear(),\n",
        "                                     nn.ReLU(),\n",
        "                                     nn.Linear(),\n",
        "                                     nn.ReLU())\n",
        "        \n",
        "        ### -------------- TODO --------------- ###\n",
        "        self.decoder =  \n",
        "\n",
        "\n",
        "    def zero_bias(self,):\n",
        "        self.mu_layer.bias.data.fill_(0.0)\n",
        "        self.logsigm_layer.bias.data.fill_(0.0)\n",
        "        \n",
        "    def forward(self, input):\n",
        "\n",
        "        # encoder forward\n",
        "        doc_vec = self.encoder(input)\n",
        "        mu = self.mu_layer(doc_vec)\n",
        "        logsigm = self.logsigm_layer(doc_vec)\n",
        "        \n",
        "        # reparameterisation\n",
        "        ### -------------- TODO --------------- ###\n",
        "        eps = \n",
        "        z =\n",
        "\n",
        "        # decoder forward\n",
        "        logits = self.decoder(z)\n",
        "        \n",
        "        # reconsrtuction loss\n",
        "        ### -------------- TODO --------------- ###\n",
        "        recons = \n",
        "        \n",
        "        # kl-divergence loss\n",
        "        ### -------------- TODO --------------- ###\n",
        "        kld = \n",
        "        \n",
        "        loss = torch.mean(recons + kld)\n",
        "        recons = torch.mean(recons)\n",
        "        kld = torch.mean(kld)\n",
        "\n",
        "        # print(loss, recons, kld)\n",
        "        \n",
        "\n",
        "        return loss, recons, kld"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "we2gyAVH-7Kn"
      },
      "source": [
        "### Question 2: Finish the Training File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0x-2Apf-7Km"
      },
      "source": [
        "def main_train():\n",
        "    num_epoch = 20\n",
        "    batch_size = 64\n",
        "    vocab_size = len(vocab)\n",
        "    n_hidden = 256\n",
        "    n_topic = 50\n",
        "    learning_rate = 0.0001\n",
        "    alternate_epochs = 5\n",
        "    \n",
        "    train_set, train_count = data_set(train_doc)\n",
        "    \n",
        "    ### -------------- TODO --------------- ###\n",
        "    model = \n",
        "    \n",
        "    model.zero_bias()\n",
        "    model.to(DEVICE)\n",
        "\n",
        "\n",
        "    ### -------------- TODO --------------- ###\n",
        "    optimizer_enc = torch.optim.Adam(,\n",
        "                                     lr = learning_rate,\n",
        "                                     eps= 1e-8)\n",
        "    optimizer_dec = torch.optim.Adam(, \n",
        "                                     lr = learning_rate,\n",
        "                                     eps= 1e-8)\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        train_batches = create_batches(len(train_set), batch_size, shuffle=True)\n",
        "        model.train() \n",
        "        \n",
        "        ### -------------- TODO --------------- ###\n",
        "        # Question: why do we need two optimizers #\n",
        "        for switch in range(0, 2): \n",
        "            if switch == 0:\n",
        "                optimizer = optimizer_dec\n",
        "                print_mode = 'updating decoder'\n",
        "            else:\n",
        "                optimizer = optimizer_enc\n",
        "                print_mode = 'updating encoder'\n",
        "                \n",
        "            loss_epoch = 0.0\n",
        "            recons_epoch = 0.0\n",
        "            kld_epoch = 0.0\n",
        "            count = 0\n",
        "    \n",
        "            for i in range(alternate_epochs):\n",
        "                                 \n",
        "                for idx_batch in train_batches:\n",
        "                    data_batch, count_batch, mask = fetch_data(train_set, train_count, idx_batch, vocab_size)\n",
        "                    input = torch.from_numpy(data_batch).float().to(DEVICE)\n",
        "                    loss, recons, kld = model(input)\n",
        "                    \n",
        "                    # optimize\n",
        "                    optimizer.zero_grad()      \n",
        "                    loss.backward()        \n",
        "                    optimizer.step()        \n",
        "                    loss_epoch += loss\n",
        "                    recons_epoch += recons\n",
        "                    kld_epoch += kld\n",
        "                    count += 1\n",
        "\n",
        "            print(f'Epoch {epoch}, loss={loss_epoch/count}, recons={recons_epoch/count}, kld={kld_epoch/count}')\n",
        "\n",
        "    return model\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QDZraKb1-7Kn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a4c0a75-829d-479a-eaf7-d353f821e4a2"
      },
      "source": [
        "model = main_train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Dsk3rJC_NKX"
      },
      "source": [
        "# save model to use in Q4\n",
        "torch.save(model.state_dict(), \"vae.pt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UBJ4Mqyb-7Kn"
      },
      "source": [
        "### Question 3: Code qualitative analysis for topics (p(x|z))\n",
        "Now that we have the VAE trained with 50 candidate topics, we can explore how the VAE model cluster words with similar topics together.\n",
        "\n",
        "In the following section, you will also need to evaluate the perplexity of the VAE model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CtFfNW5b-7Kn"
      },
      "source": [
        "#Add meta information (authors, time, geolocation etc.) to improve quality of the topics\n",
        "\n",
        "associations = {\n",
        "    'jesus': ['prophet', 'jesus', 'matthew', 'christ', 'worship', 'church'],\n",
        "    'comp ': ['floppy', 'windows', 'microsoft', 'monitor', 'workstation', 'macintosh', \n",
        "              'printer', 'programmer', 'colormap', 'scsi', 'jpeg', 'compression'],\n",
        "    'car  ': ['wheel', 'tire'],\n",
        "    'polit': ['amendment', 'libert', 'regulation', 'president'],\n",
        "    'crime': ['violent', 'homicide', 'rape'],\n",
        "    'midea': ['lebanese', 'israel', 'lebanon', 'palest'],\n",
        "    'sport': ['coach', 'hitter', 'pitch'],\n",
        "    'gears': ['helmet', 'bike'],\n",
        "    'nasa ': ['orbit', 'spacecraft'],\n",
        "}\n",
        "def identify_topic_in_line(line):\n",
        "    topics = []\n",
        "    for topic, keywords in associations.items():\n",
        "        for word in keywords:\n",
        "            if word in line:\n",
        "                topics.append(topic)\n",
        "                break\n",
        "    return topics\n",
        "\n",
        "def print_top_words(beta, feature_names, n_top_words=10):\n",
        "    print('---------------Printing the Topics------------------')\n",
        "    for i in range(len(beta)):\n",
        "        line = \" \".join([feature_names[j][0] for j in beta[i].argsort()[:-n_top_words - 1:-1]])\n",
        "        topics = identify_topic_in_line(line)\n",
        "        print('|'.join(topics))\n",
        "        print('     {}'.format(line))\n",
        "    print('---------------End of Topics------------------')\n",
        "\n",
        "\n",
        "def print_perp(model):\n",
        "    cost=[]\n",
        "    model.eval()\n",
        "    test_set, test_count = data_set(test_doc)\n",
        "    test_batches = create_batches(len(test_set), 64)\n",
        "    \n",
        "    ### -------------- TODO --------------- ###\n",
        "    \n",
        "    ppl = \n",
        "    print('The approximated perplexity is: ', ppl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# perplexity on test data\n",
        "print_perp(model)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "72hnyu6EHMM2",
        "outputId": "65724555-c27b-4d44-bcf3-d12d8f1eb945"
      },
      "source": [
        "# model latent topics\n",
        "emb = model.decoder[0].weight.data.cpu().numpy().T\n",
        "print_top_words(emb, sorted(vocab.items(), key=lambda x:x[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iw_8gul-7Kn"
      },
      "source": [
        "### Question 4: Use Topics to do Classification\n",
        "In this section, you will use both the article and the labels to train a topic classifier. Firstly, you may train a vanilla classifier, and you are likely to get around 83% validation accuracy with a vocabulary of 50k (the accuracy might be lower with a small vocabulary). Then you can use the pre-trained VAE encoder as the classifier encoder and fine-tune it to see what happens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xU4DjHwZa2W",
        "outputId": "b19009ca-d2f0-4ed0-ea30-c7dd279f69ee"
      },
      "source": [
        "'''\n",
        "Uncomment these lines to train the classifier on larger vocabulary\n",
        "NOTE: if you want to use the pre-trained VAE encoder, the vocab size for the classifier should be the same as the VAE model\n",
        "'''\n",
        "\n",
        "# vocab = {k:v for k,v in vocab_total.items() if v > 3}\n",
        "# vocab_size = len(vocab)\n",
        "# print(\"Vocabulary size after filtering:\", vocab_size)\n",
        "# word2idx = {k:n for n,(k,v) in enumerate(vocab.items())}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IPEoNEUAodhk"
      },
      "source": [
        "def fetch_labelled_data(data, labels, idx_batch, vocab_size):\n",
        "    \"\"\"fetch input data and labels by batch.\"\"\"\n",
        "    batch_size = len(idx_batch)\n",
        "    data_batch = np.zeros((batch_size, vocab_size))\n",
        "    label_batch = []\n",
        "\n",
        "    texts_batch = [data[i] for i in idx_batch]\n",
        "    label_batch = [labels[i] for i in idx_batch]\n",
        "\n",
        "    for i, text in enumerate(texts_batch):\n",
        "        for word in text:\n",
        "            if word in vocab:\n",
        "                data_batch[i, word2idx[word]] += 1\n",
        "\n",
        "    return data_batch, np.array(label_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wg3DQ0uB-7Ko"
      },
      "source": [
        "class VanillaClassifier(nn.Module):\n",
        "    def __init__(self, input_size, n_hidden, n_class, dp):\n",
        "        super().__init__()\n",
        "\n",
        "        ### --------------------- TODO ----------------------- ###\n",
        "        # construct a same encoder architecture as the VAE encoder \n",
        "        self.encoder = \n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        self.output = nn.Linear(n_hidden, n_class, bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        doc_vec = self.dropout(self.encoder(input))\n",
        "        logits = self.output(doc_vec)\n",
        "        return logits\n",
        "\n",
        "class VAEClassifier(nn.Module):\n",
        "\n",
        "    def __init__(self, vae, n_class, dp):\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = vae.encoder\n",
        "        self.vae_output = vae.n_hidden\n",
        "        \n",
        "        self.dropout = nn.Dropout(dp)\n",
        "        self.output = nn.Linear(self.vae_output, n_class, bias=True)\n",
        "\n",
        "    def forward(self, input):\n",
        "        doc_vec = self.dropout(self.encoder(input))\n",
        "\n",
        "        logits = self.output(doc_vec)\n",
        "\n",
        "        return logits"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouucz820EQwa"
      },
      "source": [
        "def evaluate(classifier, idx_batches, data, labels, vocab_size, criterion):\n",
        "    with torch.no_grad():\n",
        "        total_loss = 0\n",
        "        total_acc = 0\n",
        "        val_count = 0\n",
        "\n",
        "        for idx_batch in idx_batches:\n",
        "            ### --------------------- TODO ----------------------- ###\n",
        "            # compute validation loss and accuracy\n",
        "\n",
        "    return total_loss/val_count, total_acc/val_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY39UKMPojYV"
      },
      "source": [
        "def cls_train(train_data, train_labels, valid_data, valid_labels, vocab_size):\n",
        "    num_epoch = 10\n",
        "    batch_size = 64\n",
        "    dropout = 0.1\n",
        "    n_hidden = 64\n",
        "    learning_rate = 0.0001\n",
        "    \n",
        "    # model.load_state_dict(torch.load(\"vae.pt\"))\n",
        "    # classifier = VAEClassifier(model, 20, dropout)\n",
        "    classifier = VanillaClassifier(vocab_size, n_hidden, 20, dropout)\n",
        "\n",
        "    classifier.to(DEVICE)\n",
        "\n",
        "    optimizer = torch.optim.Adam(classifier.parameters(),\n",
        "                                     lr = learning_rate)\n",
        "    \n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    for epoch in range(num_epoch):\n",
        "        train_batches = create_batches(len(train_prep), batch_size, shuffle=True)\n",
        "        valid_batches = create_batches(len(test_prep), batch_size, shuffle=False)\n",
        "        \n",
        "                \n",
        "        train_loss = 0.0\n",
        "        count = 0\n",
        "        acc = 0\n",
        "        classifier.train()              \n",
        "        for idx_batch in train_batches:\n",
        "            optimizer.zero_grad()\n",
        "            ### --------------------- TODO ----------------------- ###\n",
        "            # finish training loop\n",
        "\n",
        "            \n",
        "            count += 1\n",
        "        \n",
        "        # validation\n",
        "        classifier.eval()\n",
        "        valid_loss, valid_acc = evaluate(classifier, valid_batches, valid_data, test_labels, vocab_size, criterion)\n",
        "        print(f'Epoch {epoch},\\ttrain_loss={train_loss/count:.3f},\\ttrain_acc={acc/count:.3f},\\tvalid_loss={valid_loss:.3f},\\tvalid_acc={valid_acc:.3f}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6CR2PohDC2B",
        "outputId": "18fbdea8-cd27-4481-ca4f-9b71b2de7c4e"
      },
      "source": [
        "cls_train(train_prep, train_labels, test_prep, test_labels, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ft3SMu7vXsjI"
      },
      "source": [
        "Since the training of VAE takes a long time, the vocabulary was truncated into 5000 words. Therefore, the benefit from VAE pre-training might not seem very obvious. You may try and train a new VAE with a larger vocabulary (e.g. 50k and train for 20 epochs), it would help in classifing the topics.\n",
        "\n",
        "In addition, the validation accuracy might be low if your vocabulary size is only 5,000 (valid acc around 65%). The below code is just a baseline utilizing all the words, and it would achieve around 83% accuracy on the validation set."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCVaP-MhjoXY",
        "outputId": "051ad19b-66e8-4744-959c-fd8f09a6aa9b"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "vectorizer = CountVectorizer()\n",
        "train_features = vectorizer.fit_transform(train_data)\n",
        "test_features = vectorizer.transform(test_data)\n",
        "\n",
        "train_labels = np.array(train_news_group['target'])\n",
        "test_labels = np.array(test_news_group['target'])\n",
        "\n",
        "vocab_size = len(vectorizer.vocabulary_)\n",
        "print(vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBG5Xxuy1zmX"
      },
      "source": [
        "def fetch_labelled_data(features, labels, idx_batch, vocab_size=None):\n",
        "    idxs = np.array(idx_batch)\n",
        "\n",
        "    feature_batch = features[idxs, :].toarray()\n",
        "    label_batch = labels[idxs]\n",
        "    return feature_batch, label_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJIIc5pPW_Qb",
        "outputId": "2feb467d-8798-4546-b887-12a7b9c881ab"
      },
      "source": [
        "cls_train(train_features, train_labels, test_features, test_labels, vocab_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MKSA5sl6XFSh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}