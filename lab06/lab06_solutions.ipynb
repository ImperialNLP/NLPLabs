{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.1"
    },
    "colab": {
      "name": "lab07_transformers_solutions (2).ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "military-character"
      },
      "source": [
        "## Introduction"
      ],
      "id": "military-character"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EslAmsE-tWMB"
      },
      "source": [
        "In this lab, we will take you through a practical use of Transformers. This notebook shows you how to use [Hugging face](https://huggingface.co/)'s package to import and train pretrained models for the tasks of hate speech classification and machine translation.\r\n",
        "\r\n",
        "We first show you all necessay components to use the ``transformers`` package before asking you to implement some code in the later sections.\r\n",
        "\r\n",
        "\r\n",
        "**Note:** The training of models will take quite some time so make sure to run this session with the GPU enabled. \r\n"
      ],
      "id": "EslAmsE-tWMB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rwXjbNUJHzZ0"
      },
      "source": [
        "## Setting up the Environment"
      ],
      "id": "rwXjbNUJHzZ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vina6dhaHz1N"
      },
      "source": [
        "First, we need to install Hugging Face [transformers](https://huggingface.co/transformers/index.html) and [Sentence piece Tokenizers](https://github.com/google/sentencepiece) with the following commands"
      ],
      "id": "Vina6dhaHz1N"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "other-scottish"
      },
      "source": [
        "#! pip install torch"
      ],
      "id": "other-scottish",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "modern-olympus"
      },
      "source": [
        "!pip install transformers\r\n",
        "!pip install sentencepiece\r\n",
        "!pip install ipywidgets\r\n",
        "!jupyter nbextension enable --py widgetsnbextension"
      ],
      "id": "modern-olympus",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "developing-france"
      },
      "source": [
        "import torch\n",
        "import transformers\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "from transformers import Trainer, TrainingArguments\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertPreTrainedModel, BertModel\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "if not torch.cuda.is_available():\n",
        "  print('WARNING: You may want to change the runtime to GPU for faster training!')\n",
        "  DEVICE = 'cpu'\n",
        "else:\n",
        "  DEVICE = 'cuda:0'"
      ],
      "id": "developing-france",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58jPlYaWk7fh"
      },
      "source": [
        "If you work in Colab, mount your google drive to save models and training checkpoints. Run the following code to connect your google drive to colab. Click on the link and copy and past the code you saw into the input box."
      ],
      "id": "58jPlYaWk7fh"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qjdWhQHBk6RW"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')\r\n",
        "\r\n",
        "%cd '/content/drive/MyDrive/Colab Notebooks/'\r\n",
        "%mkdir './Lab 7'\r\n",
        "%cd './Lab 7' "
      ],
      "id": "qjdWhQHBk6RW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJ-jH0JoVG1v"
      },
      "source": [
        "# Hate Speech Classification"
      ],
      "id": "qJ-jH0JoVG1v"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cognitive-syndicate"
      },
      "source": [
        "## Downloading the dataset\r\n",
        "\r\n",
        "For the task of hate speech classification, we will work with the [Offensive Language Identification Dataset - OLID ](https://scholar.harvard.edu/malmasi/olid). It is a dataset of tweets hierarchically annotated on three levels: \r\n",
        "\r\n",
        "* Level A: Offensive Language Detection\r\n",
        "* Level B: Categorization of Offensive Language\r\n",
        "* Level C: Offensive Language Target Identification\r\n",
        "\r\n",
        "\r\n",
        "Let's download it first."
      ],
      "id": "cognitive-syndicate"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eISPHZ-7HwBk"
      },
      "source": [
        "%mkdir ./data\r\n",
        "%cd ./data\r\n",
        "\r\n",
        "if not os.path.isfile('pretrain.txt'): \r\n",
        "  !wget -O pretrain.txt https://www.dropbox.com/s/bavjtyx0ndty7xt/pretrain.txt?dl=0\r\n",
        "\r\n",
        "if not os.path.isfile('OLIDv1.0.zip'): \r\n",
        "  !wget -O OLIDv1.0.zip https://sites.google.com/site/offensevalsharedtask/olid/OLIDv1.0.zip?attredirects=0&d=1\r\n",
        "  ! unzip OLIDv1.0.zip\r\n",
        "  \r\n",
        "%cd ..\r\n"
      ],
      "id": "eISPHZ-7HwBk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsIt3OVmNnRY"
      },
      "source": [
        "Let's have a look at the data we downloaded.\r\n",
        "\r\n",
        "As mentioned above, the ``OLID`` dataset has been labeled for three subtask, therefore we have three different labels sets per tweet: \r\n",
        "* Task A: Not Offensive (``NOT``) and Offensive (``OFF``).\r\n",
        "* Task B: Targeted Insult (``TIN``), Untargeted (``UNT``) and ``NULL`` for not offensive tweets.\r\n",
        "* Task C: Individual (``IND``), Group (``GRP``), Other (``OTH``) and ``NULL`` for not offensive and non targeted tweets."
      ],
      "id": "TsIt3OVmNnRY"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFwKBesVNmmr"
      },
      "source": [
        "df = pd.read_csv('./data/olid-training-v1.0.tsv',delimiter=\"\\t\")\r\n",
        "\r\n",
        "print(f'Number of training samples: {len(df)}')\r\n",
        "\r\n",
        "df.head()"
      ],
      "id": "rFwKBesVNmmr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "published-southeast"
      },
      "source": [
        "## Loading and preprocessing the corpus \r\n"
      ],
      "id": "published-southeast"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw6m7B6rEL6E"
      },
      "source": [
        "Let's define ``reader_train`` and ``reader_test`` that will prepare our data corpus and labels for both train and test set."
      ],
      "id": "nw6m7B6rEL6E"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "correct-tuner"
      },
      "source": [
        "def reader_train(file_name):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    fin = open(file_name)\n",
        "    title = fin.readline()\n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    while True:\n",
        "        line = fin.readline()\n",
        "        if not line:\n",
        "            break\n",
        "        items = line.split('\\t')\n",
        "        text = items[1]\n",
        "        label_a = set_a.index(items[2].strip())\n",
        "        label_b = set_b.index(items[3].strip())\n",
        "        label_c = set_c.index(items[4].strip())\n",
        "\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}"
      ],
      "id": "correct-tuner",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "designed-screening"
      },
      "source": [
        "def reader_test(test_textlist, test_labellist):\n",
        "    texts = []\n",
        "    labels = []\n",
        "    text_dict = {}\n",
        "    \n",
        "    # build text_dict\n",
        "    for file_text in test_textlist:\n",
        "        fin = open(file_text)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split('\\t')\n",
        "            if items[0] not in text_dict:\n",
        "                text_dict[items[0]] = items[1]\n",
        "        fin.close()\n",
        "    label_dict_list = []\n",
        "    \n",
        "    # build label_dict\n",
        "    for i, file_label in enumerate(test_labellist):\n",
        "        label_dict_list.append({})\n",
        "        fin = open(file_label)\n",
        "        title = fin.readline()\n",
        "        while True:\n",
        "            line = fin.readline()\n",
        "            if not line:\n",
        "                break\n",
        "            items = line.split(',')\n",
        "            label_dict_list[i][items[0]] = items[1]\n",
        "        fin.close()    \n",
        "    \n",
        "    set_a = ['NOT' , 'OFF']\n",
        "    set_b = ['NULL', 'TIN', 'UNT']\n",
        "    set_c = ['NULL', 'IND', 'GRP', 'OTH']\n",
        "    \n",
        "    for idx, text in text_dict.items():\n",
        "        if len(text) > 0:\n",
        "            texts.append(text)\n",
        "            if idx in label_dict_list[0]:\n",
        "                label_a = label_dict_list[0][idx]\n",
        "            else:\n",
        "                label_a = 'OFF'\n",
        "            if idx in label_dict_list[1]:\n",
        "                label_b = label_dict_list[1][idx]\n",
        "            else:\n",
        "                label_b = 'NULL'\n",
        "            if idx in label_dict_list[2]:\n",
        "                label_c = label_dict_list[2][idx]\n",
        "            else:\n",
        "                label_c = 'NULL'\n",
        "            \n",
        "            label_a = set_a.index(label_a.strip())\n",
        "            label_b = set_b.index(label_b.strip())\n",
        "            label_c = set_c.index(label_c.strip())\n",
        "        \n",
        "            labels.append([label_a, label_b, label_c])\n",
        "            \n",
        "    return {'texts':texts, 'labels':labels}            \n"
      ],
      "id": "designed-screening",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4kLz8nca7ix"
      },
      "source": [
        "We also define our custom ``OlidDataset`` class which allows us to control how we handle the iteration and batches.\r\n",
        "\r\n",
        "At each iteration over the dataset object, the function ``__get_item__`` is called and returns a list of dictionnaries with the tweets and their 3 labels. \r\n",
        "Then, the ``collate_fn`` function will process the list of samples into their encodings and return a batch when called by the iterator during training."
      ],
      "id": "T4kLz8nca7ix"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amber-exposure"
      },
      "source": [
        "class OlidDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_set):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "        self.texts = input_set['texts']\n",
        "        self.labels = input_set['labels']\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "\n",
        "        texts = []\n",
        "        labels_a = []\n",
        "        labels_b = []\n",
        "        labels_c = []\n",
        "\n",
        "        for b in batch:\n",
        "            texts.append(b['text'])\n",
        "            labels_a.append(b['label_a'])\n",
        "            labels_b.append(b['label_b'])\n",
        "            labels_c.append(b['label_c'])\n",
        "\n",
        "        #The maximum sequence size for BERT is 512 but here the tokenizer truncate sentences longer than 128 tokens.  \n",
        "        # We also pad shorter sentences to a length of 128 tokens\n",
        "        encodings = self.tokenizer(texts, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "        labels = {}\n",
        "        encodings['label_a'] =  torch.tensor(labels_a)\n",
        "        encodings['label_b'] =  torch.tensor(labels_b)\n",
        "        encodings['label_c'] =  torch.tensor(labels_c)\n",
        "        \n",
        "        return encodings\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "       \n",
        "        item = {'text': self.texts[idx],\n",
        "                'label_a': self.labels[idx][0],\n",
        "                'label_b': self.labels[idx][1],\n",
        "                'label_c': self.labels[idx][2]}\n",
        "        return item"
      ],
      "id": "amber-exposure",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66CvfwkHbXO_"
      },
      "source": [
        "\r\n",
        "Now let's put it all together and load our data. Here we use a pre-made tokenizer that was used for our BERT model. Here we pick the pre-trained model ``bert-base-cased``. There are several other models of various sizes (base, large).\r\n",
        "\r\n",
        "**Note:** ``bert-base-cased`` is case-sensitive and it differenciates English from english. An non case-sensitive variant is ``bert-base-uncased``.\r\n",
        "\r\n",
        "You can always use another [tokenizer](https://huggingface.co/transformers/main_classes/tokenizer.html), but we will get better results using the same tokenizer as the one used to pre-train the model.\r\n"
      ],
      "id": "66CvfwkHbXO_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nHzy19X9TUqc"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
        "\r\n",
        "# we can check the parameters of this tokenizer\r\n",
        "tokenizer"
      ],
      "id": "nHzy19X9TUqc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deluxe-biography"
      },
      "source": [
        "trainset = reader_train('./data/olid-training-v1.0.tsv')\n",
        "testset = reader_test(['./data/testset-levela.tsv','./data/testset-levelb.tsv','./data/testset-levelc.tsv'], \n",
        "                      ['./data/labels-levela.csv','./data/labels-levelb.csv','./data/labels-levelc.csv'])\n",
        "\n",
        "train_dataset = OlidDataset(tokenizer, trainset)\n",
        "test_dataset = OlidDataset(tokenizer, testset)"
      ],
      "id": "deluxe-biography",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oxEXetnGKVj2"
      },
      "source": [
        "The following code let's you play around with our ``train_dataset`` object."
      ],
      "id": "oxEXetnGKVj2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Q2VvuoaKFXB"
      },
      "source": [
        "#returns first item as dictionnary\r\n",
        "#print(train_dataset[0])\r\n",
        "\r\n",
        "# put all train set into one batch for the collate_fn function\r\n",
        "batch = [sample for sample in train_dataset]\r\n",
        "\r\n",
        "encodings = train_dataset.collate_fn(batch[:10])\r\n",
        "\r\n",
        "for key, value in encodings.items():\r\n",
        "  print(f\"{key}: {value.numpy().tolist()}\")\r\n",
        "\r\n"
      ],
      "id": "1Q2VvuoaKFXB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "opponent-closure"
      },
      "source": [
        "## Finetuning a pre-trained BERT model"
      ],
      "id": "opponent-closure"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpaMPoB5cNwt"
      },
      "source": [
        "\r\n",
        "As you can recall from the lecture, BERT is a model trained on Masked language Modeling(MLM) and Next Sentence Prediction(NSP), however is not trained to do to do sentence classification. We then need to adapt it for hate speech classification and finetune the pre-trained model on our dataset.\r\n",
        "\r\n",
        "\r\n"
      ],
      "id": "dpaMPoB5cNwt"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n2zPF-H3nHDH"
      },
      "source": [
        "Let's have a look at ``bert_base-uncased`` summary."
      ],
      "id": "n2zPF-H3nHDH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk_QEpoGnZhL"
      },
      "source": [
        "model = BertModel.from_pretrained(\"bert-base-cased\")\r\n",
        "\r\n",
        "#180 M\r\n",
        "print(f\"Model size: {model.num_parameters()}\")\r\n",
        "\r\n",
        "#model summary\r\n",
        "model"
      ],
      "id": "pk_QEpoGnZhL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GBOHvhd-n00L"
      },
      "source": [
        "Note that the model has only encoder layers."
      ],
      "id": "GBOHvhd-n00L"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ESBsGVb_e2QT"
      },
      "source": [
        "### BERT Model\r\n",
        "\r\n",
        "To define our model, we will build on top of a Huggingface pre-trained model and adapt it to our task. We will use ``BertModel`` to extract embeddings and add a ``Linear`` layer to classify samples. Hugging face implementation of BERT can handle different variations of the model, which we define and pass its parameter values via``config``.\r\n"
      ],
      "id": "ESBsGVb_e2QT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "feb9AN0YFWDe"
      },
      "source": [
        "The code below defines a model adapted to classify tweets on Level A, Offensive Language Detection. We will implement Task B and C later.\r\n",
        "\r\n"
      ],
      "id": "feb9AN0YFWDe"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bottom-tribute"
      },
      "source": [
        "class BERT_hate_speech(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        # Task B\n",
        "        # TBA\n",
        "        \n",
        "        # Task C\n",
        "        # TBA\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        " \n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Logits A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        return logits_a\n"
      ],
      "id": "bottom-tribute",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMyL5gGKfECs"
      },
      "source": [
        "### Finetuning"
      ],
      "id": "yMyL5gGKfECs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QaKWwqYMlCVX"
      },
      "source": [
        "Finally, we should define our training loop. Fortunately, the ``transformers`` package provides us with a [``Trainer``](https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer) class wich takes care of the training of transformers models.\r\n"
      ],
      "id": "QaKWwqYMlCVX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjljHz4nlAbJ"
      },
      "source": [
        "We build our custom ``Trainer`` class to incorporate our own ``compute_loss`` function over the three labels. "
      ],
      "id": "MjljHz4nlAbJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "engaged-perspective"
      },
      "source": [
        "\n",
        "class Trainer_hate_speech(Trainer):\n",
        "    def compute_loss(self, model, inputs):\n",
        "        labels = {}\n",
        "        labels['label_a'] = inputs.pop('label_a')\n",
        "        labels['label_b'] = inputs.pop('label_b')\n",
        "        labels['label_c'] = inputs.pop('label_c')\n",
        "\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # TASK A\n",
        "        loss_task_a = nn.CrossEntropyLoss()\n",
        "        labels_a = labels['label_a']\n",
        "        loss_a = loss_task_a(outputs.view(-1, 2), labels_a.view(-1))\n",
        "\n",
        "        loss = loss_a\n",
        "        \n",
        "        return loss"
      ],
      "id": "engaged-perspective",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0WDX456F4YB"
      },
      "source": [
        "\r\n",
        "Now let's finetune the pretrained model on our ``OlidDataset``.\r\n",
        "\r\n",
        "In our function ``main_hate_speech``we define the arguments for the ``Trainer`` object and launch the training with ``trainer.train``. \r\n"
      ],
      "id": "p0WDX456F4YB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "killing-population"
      },
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    #call our custom BERT model and pass as parameter the name of an available pretrained model\n",
        "    model = BERT_hate_speech.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 3,\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                   \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_finetuned/')\n",
        "\n"
      ],
      "id": "killing-population",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OanfT22IduY6"
      },
      "source": [
        "Let's run it."
      ],
      "id": "OanfT22IduY6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "looking-escape"
      },
      "source": [
        "main_hate_speech()"
      ],
      "id": "looking-escape",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q0sYQeoM90U"
      },
      "source": [
        "### Evaluation\r\n",
        "Once we trained our model, we can evaluate it on our test set."
      ],
      "id": "6Q0sYQeoM90U"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tt8KRJs2OSo1"
      },
      "source": [
        "Let's define a helper function ``predict_hatespeech`` that will extract the predicted label."
      ],
      "id": "tt8KRJs2OSo1"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leJGKWvXGDf7"
      },
      "source": [
        "def predict_hatespeech(input, tokenizer, model): \r\n",
        "  model.eval()\r\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\r\n",
        "  \r\n",
        "  output = model(**encodings)\r\n",
        "  preds = torch.max(output, 1)\r\n",
        "\r\n",
        "  return {'prediction':preds[1], 'confidence':preds[0]}"
      ],
      "id": "leJGKWvXGDf7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_y2mgB3-PQpI"
      },
      "source": [
        "Now let's define a function that will evaluate our model on the test set we prepared."
      ],
      "id": "_y2mgB3-PQpI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2olq7egUHYnH"
      },
      "source": [
        "def evaluate(model, tokenizer, data_loader):\r\n",
        "\r\n",
        "  total_count = 0\r\n",
        "  correct_count = 0 \r\n",
        "\r\n",
        "  preds = []\r\n",
        "  tot_labels = []\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for data in tqdm(data_loader): \r\n",
        "\r\n",
        "      labels = {}\r\n",
        "      labels['label_a'] = data['label_a']\r\n",
        "\r\n",
        "      tweets = data['text']\r\n",
        "\r\n",
        "      pred = predict_hatespeech(tweets, tokenizer, model)\r\n",
        "\r\n",
        "      preds.append(pred['prediction'])\r\n",
        "      tot_labels.append(labels['label_a'])\r\n",
        "\r\n",
        "  # with the saved predictions and labels we can compute accuracy, precision, recall and f1-score\r\n",
        "  report = classification_report(tot_labels, preds, target_names=[\"Not offensive\",\"Offensive\"], output_dict= True)\r\n",
        "\r\n",
        "  return report"
      ],
      "id": "2olq7egUHYnH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sps-kXvWeNoO"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
        "\r\n",
        "#your saved model name here\r\n",
        "model_name = './models/ht_bert_finetuned/' \r\n",
        "model = BERT_hate_speech.from_pretrained(model_name)\r\n",
        "\r\n",
        "# we don't batch our test set unless it's too big\r\n",
        "test_loader = DataLoader(test_dataset)\r\n",
        "\r\n",
        "report = evaluate(model, tokenizer, test_loader)\r\n",
        "\r\n",
        "print(report)\r\n",
        "\r\n",
        "print(report['accuracy'])\r\n",
        "print(report['Not offensive']['f1-score'])\r\n",
        "print(report['Offensive']['f1-score'])"
      ],
      "id": "Sps-kXvWeNoO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9eJi1IaOGZ6"
      },
      "source": [
        "Let's test our model on a few sentences to get an intuition. Feel free to play around."
      ],
      "id": "g9eJi1IaOGZ6"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49umxeBiK3Ec"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
        "model = BERT_hate_speech.from_pretrained('./models/ht_bert_finetuned/')\r\n",
        "\r\n",
        "print(predict_hatespeech(\"I go see pinguins at the zoo.\", tokenizer, model))\r\n",
        "print(predict_hatespeech(\"Bananas are stupid\", tokenizer, model))"
      ],
      "id": "49umxeBiK3Ec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaD0CmXMhEmA"
      },
      "source": [
        "## Pre-training and finetuning BERT\r\n",
        "\r\n",
        "In this section, we will implement our own masked language modeling (MLM) training."
      ],
      "id": "JaD0CmXMhEmA"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmT9ztohfiW3"
      },
      "source": [
        "### Pre-training"
      ],
      "id": "KmT9ztohfiW3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-mAsIUYZ7Ex"
      },
      "source": [
        "**Question 1: Add MLM head for pretraining**\r\n",
        "Your task is to fill in the following classes to implement MLM training: \r\n",
        "\r\n",
        "* ``PretrainDataset()``\r\n",
        "* ``Trainer_MLM()``\r\n",
        "* ``BERT_pretrain()``\r\n",
        "* ``main_pretrain()``"
      ],
      "id": "9-mAsIUYZ7Ex"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-3Jvmryw-ko"
      },
      "source": [
        "To train our model in a MLM fashion, we need to make some adjustment to our ``Dataset`` class. We want to train BERT to predict an X% of tokens (in the original paper it is 15%) of which 80% will be replaced by a ``[MASK]`` token, 10% with a random token and 10% remain the same token.\r\n",
        "\r\n",
        "We introduce the function ``mask_tokens`` that will take care of that."
      ],
      "id": "C-3Jvmryw-ko"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "differential-ordinary"
      },
      "source": [
        "class PretrainDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, tokenizer, input_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "        self.texts = self.read_text(input_file)\n",
        "\n",
        "        self.mlm_probability = 0.15\n",
        "        \n",
        "    def read_text(self, input_file):\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        fin = open(input_file)\n",
        "        return fin.readlines()\n",
        "        \n",
        "    def collate_fn(self, batch):\n",
        "       \n",
        "        ## Question 1 ##\n",
        "\n",
        "        batch = self.tokenizer(batch, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
        "\n",
        "        inputs, labels = self.mask_tokens(batch[\"input_ids\"])\n",
        "        return {\"input_ids\": inputs, \"labels\": labels}\n",
        "    \n",
        "        return encodings\n",
        "    \n",
        "    def mask_tokens(self, inputs):\n",
        "        \"\"\"\n",
        "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
        "        \"\"\"\n",
        "        if self.tokenizer.mask_token is None:\n",
        "            raise ValueError(\n",
        "                \"This tokenizer does not have a mask token which is necessary for masked language modeling. Remove the --mlm flag if you want to use this tokenizer.\"\n",
        "            )\n",
        "        labels = inputs.clone()\n",
        "\n",
        "        # We sample a few tokens in each sequence for masked-LM training (with probability args.mlm_probability defaults to 0.15 in Bert/RoBERTa)\n",
        "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
        "        special_tokens_mask = [\n",
        "            self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in labels.tolist()\n",
        "        ]\n",
        "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
        "        \n",
        "        if self.tokenizer._pad_token is not None:\n",
        "            padding_mask = labels.eq(self.tokenizer.pad_token_id)\n",
        "            probability_matrix.masked_fill_(padding_mask, value=0.0)\n",
        "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
        "        labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
        "\n",
        "        # 80% of the time, we replace masked input tokens with tokenizer.mask_token ([MASK])\n",
        "        indices_replaced = torch.bernoulli(torch.full(labels.shape, 0.8)).bool() & masked_indices\n",
        "        inputs[indices_replaced] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
        "\n",
        "        # 10% of the time, we replace masked input tokens with random word\n",
        "        indices_random = torch.bernoulli(torch.full(labels.shape, 0.5)).bool() & masked_indices & ~indices_replaced\n",
        "        random_words = torch.randint(len(self.tokenizer), labels.shape, dtype=torch.long)\n",
        "        inputs[indices_random] = random_words[indices_random]\n",
        "\n",
        "        # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
        "        return inputs, labels\n",
        "    \n",
        "    def __len__(self):\n",
        "        \n",
        "        ## Question 1 ##\n",
        "\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        ## Question 1 ##\n",
        " \n",
        "        text = self.texts[idx]\n",
        "        return text"
      ],
      "id": "differential-ordinary",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-4yxeOG1Z94H"
      },
      "source": [
        "The next step is to add a MLM head to our model. \r\n",
        "Use the ``BertOnlyMLMHead`` to add a MLM classifier to BERT."
      ],
      "id": "-4yxeOG1Z94H"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beginning-fluid"
      },
      "source": [
        "from transformers.models.bert.modeling_bert import BertOnlyMLMHead\n",
        "\n",
        "class BERT_pretrain(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.config = config\n",
        "\n",
        "        ## Question 1 ##\n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        \n",
        "        ## Question 1 ##\n",
        "        # MLM head\n",
        "        self.cls = BertOnlyMLMHead(config)\n",
        "        \n",
        "        \n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        ## Question 1 ##\n",
        "\n",
        "        # MLM output\n",
        "        prediction_scores = self.cls(outputs[0])\n",
        "        \n",
        "        return prediction_scores"
      ],
      "id": "beginning-fluid",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46_QOzQfLgME"
      },
      "source": [
        "We will define a new Trainer class for pre-training. \r\n",
        "\r\n",
        "**Note:** We could use the standard ``Trainer`` class to train our model. Then we would need to make ``BERT_pretrain`` output  ``loss`` and BERT ``outputs`` as a tuple``(loss, outputs)``.\r\n",
        "\r\n",
        "\r\n"
      ],
      "id": "46_QOzQfLgME"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xoG-AvUdzyqt"
      },
      "source": [
        "class Trainer_MLM(Trainer):\r\n",
        "    def compute_loss(self, model, inputs):\r\n",
        "        \r\n",
        "        labels = inputs['labels']\r\n",
        "\r\n",
        "        outputs = model(**inputs)\r\n",
        "\r\n",
        "        # MLM loss\r\n",
        "        lm_loss = nn.CrossEntropyLoss()\r\n",
        "\r\n",
        "        loss_mlm = lm_loss(outputs.view(-1, model.config.vocab_size), labels.view(-1))\r\n",
        "        \r\n",
        "        loss = loss_mlm\r\n",
        "        \r\n",
        "        return loss"
      ],
      "id": "xoG-AvUdzyqt",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q3xGoHTSh-Yu"
      },
      "source": [
        "Finally, put everything together in the ``main_pretrain()`` class. \r\n",
        "\r\n",
        "In the code below, write code to pre-train your custom MLM model on ``pretrain.txt`` file found in the ``data`` folder.\r\n",
        "\r\n",
        "\r\n",
        "\r\n"
      ],
      "id": "q3xGoHTSh-Yu"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "familiar-singles"
      },
      "source": [
        "def main_pretrain():\n",
        "    \n",
        "    ## Question 1 ##\n",
        "\n",
        "    model = BERT_pretrain.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
        "    pretrain_dataset = PretrainDataset(tokenizer, 'data/pretrain.txt')\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/pretrain',\n",
        "        learning_rate = 0.00005,\n",
        "        num_train_epochs =1,\n",
        "        save_steps = 10000,  #saves a checkpoint file every 10000 iterations\n",
        "        per_device_train_batch_size=64,\n",
        "    )\n",
        "    trainer = Trainer_MLM(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=pretrain_dataset,                    \n",
        "        data_collator=pretrain_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    \n",
        "    trainer.save_model('./models/ht_bert_pretrained/')\n",
        "    "
      ],
      "id": "familiar-singles",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-tsbImSJvyf"
      },
      "source": [
        "Running the pretraining will take ~ 2 hours with one epoch."
      ],
      "id": "4-tsbImSJvyf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "greenhouse-signal"
      },
      "source": [
        " main_pretrain()"
      ],
      "id": "greenhouse-signal",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "incorporated-sleep"
      },
      "source": [
        "### Finetuning\n",
        "\n",
        "**Question 2: Load the pretrained model for finetuning**\n",
        "\n",
        "In the code below modify the ``main_hate_speech`` function from earlier to import the model we just trained, and finetune it on our ``OlidDataset`` train sets.\n",
        "\n",
        "**Note**: Your pre-trained model is saved as checkpoint files in your ``output_dir`` folder."
      ],
      "id": "incorporated-sleep"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "metropolitan-cliff"
      },
      "source": [
        "def main_hate_speech():\n",
        "\n",
        "    ## Question 2 ##\n",
        "\n",
        "    model = BERT_hate_speech.from_pretrained(\"./models/ht_bert_pretrained/\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 500,\n",
        "        per_device_train_batch_size=32,\n",
        "        num_train_epochs = 1\n",
        "    )\n",
        "    trainer = Trainer_hate_speech(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,        \n",
        "        eval_dataset=test_dataset,             \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_pretrained_finetuned/')\n"
      ],
      "id": "metropolitan-cliff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "simplified-exemption"
      },
      "source": [
        "main_hate_speech()"
      ],
      "id": "simplified-exemption",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eqtu-ECSaUO"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "8eqtu-ECSaUO"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCC3iV37M7np"
      },
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
        "\r\n",
        "#your saved model name here\r\n",
        "model_name = './models/ht_bert_pretrained_finetuned/' \r\n",
        "model = BERT_hate_speech.from_pretrained(model_name)\r\n",
        "\r\n",
        "test_loader = DataLoader(test_dataset)\r\n",
        "\r\n",
        "report = evaluate(model, tokenizer, test_loader)\r\n",
        "print(report)\r\n",
        "print(report['accuracy'])\r\n",
        "print(report['Not offensive']['f1-score'])\r\n",
        "print(report['Offensive']['f1-score'])"
      ],
      "id": "qCC3iV37M7np",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "existing-courtesy"
      },
      "source": [
        "## Multi-task Hate Speech Classification\n",
        "\n",
        "It's time to add the two other tasks to our implementation of ``BERT_hate_speech()``.\n",
        "\n",
        "**Question 3: Add multi-heads (task b, task c) for multi-task hatespeech classification**\n",
        "\n",
        "Fill in the missing code from the following classes:\n",
        "\n",
        "* ``BERT_hate_speech_multitask()``\n",
        "* `` Trainer_hate_speech_multitask()``\n",
        "* ``main_hate_speech_multitask()``"
      ],
      "id": "existing-courtesy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ogHc8w4Kyew"
      },
      "source": [
        "### Multi-task Model"
      ],
      "id": "9ogHc8w4Kyew"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "personalized-teddy"
      },
      "source": [
        "\n",
        "class BERT_hate_speech_multitask(BertPreTrainedModel):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        \n",
        "        # BERT Model\n",
        "        self.bert = BertModel(config)\n",
        "        \n",
        "        # Task A\n",
        "        self.projection_a = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 2))\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "\n",
        "        # Task B\n",
        "        self.projection_b = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 3))\n",
        "\n",
        "        # Task C\n",
        "        self.projection_c = torch.nn.Sequential(torch.nn.Dropout(0.2),\n",
        "                                                torch.nn.Linear(config.hidden_size, 4))\n",
        "        \n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        token_type_ids=None,\n",
        "        position_ids=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        labels=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None):\n",
        "\n",
        "        outputs = self.bert(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            token_type_ids=token_type_ids,\n",
        "            position_ids=position_ids,\n",
        "            head_mask=head_mask,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        # Task A\n",
        "        logits_a = self.projection_a(outputs[1])\n",
        "        \n",
        "        ##  Question 3 ##\n",
        "        \n",
        "        # Task B\n",
        "        logits_b = self.projection_b(outputs[1])\n",
        "      \n",
        "        # Task C \n",
        "        logits_c = self.projection_c(outputs[1])\n",
        "\n",
        "        return (logits_a, logits_b, logits_c)"
      ],
      "id": "personalized-teddy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwCad-uo1pnP"
      },
      "source": [
        "class Trainer_hate_speech_multitask(Trainer):\r\n",
        "    def compute_loss(self, model, inputs):\r\n",
        "        labels = {}\r\n",
        "        labels['label_a'] = inputs.pop('label_a')\r\n",
        "        labels['label_b'] = inputs.pop('label_b')\r\n",
        "        labels['label_c'] = inputs.pop('label_c')\r\n",
        "\r\n",
        "        (out_a, out_b, out_c) = model(**inputs)\r\n",
        "\r\n",
        "        # LOSS A\r\n",
        "        loss_task_a = nn.CrossEntropyLoss()\r\n",
        "        labels_a = labels['label_a']\r\n",
        "        loss_a = loss_task_a(out_a.view(-1, 2), labels_a.view(-1))\r\n",
        "\r\n",
        "        ## QUESTION 3 ##        \r\n",
        "        # LOSS B\r\n",
        "        loss_task_b = nn.CrossEntropyLoss()\r\n",
        "        labels_b = labels['label_b']\r\n",
        "        loss_b = loss_task_b(out_b.view(-1, 3), labels_b.view(-1))\r\n",
        "\r\n",
        "        # LOSS C\r\n",
        "        loss_task_c = nn.CrossEntropyLoss()\r\n",
        "        labels_c = labels['label_c']\r\n",
        "        loss_c = loss_task_c(out_c.view(-1, 4), labels_c.view(-1))\r\n",
        "\r\n",
        "        loss = loss_a + loss_b + loss_c\r\n",
        "        \r\n",
        "        return loss"
      ],
      "id": "lwCad-uo1pnP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSA-HdbWjpAl"
      },
      "source": [
        "Just as in the finetuning task, instantiate a ``BERT_hate_speech_multitask`` model from an pre-trained model and finetune it on our ``train_dataset``."
      ],
      "id": "LSA-HdbWjpAl"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "satisfied-short"
      },
      "source": [
        "def main_hate_speech_multitask():\n",
        "    ##  Question 3 ##\n",
        "\n",
        "    model = BERT_hate_speech_multitask.from_pretrained(\"bert-base-cased\")\n",
        "    \n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/hate_speech_multitask',\n",
        "        learning_rate = 0.0001,\n",
        "        logging_steps= 100,\n",
        "        num_train_epochs = 3,\n",
        "        per_device_train_batch_size=64,\n",
        "    )\n",
        "    trainer = Trainer_hate_speech_multitask(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=train_dataset,                 \n",
        "        data_collator=train_dataset.collate_fn\n",
        "    )\n",
        "    trainer.train()\n",
        "\n",
        "    trainer.save_model('./models/ht_bert_multi_finetuned/')"
      ],
      "id": "satisfied-short",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-P1ekYHaqPwR"
      },
      "source": [
        "Running the code below should take ~10 min for 3 epochs."
      ],
      "id": "-P1ekYHaqPwR"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrapped-trading"
      },
      "source": [
        "main_hate_speech_multitask()"
      ],
      "id": "wrapped-trading",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p2EFyb3rKtVB"
      },
      "source": [
        "### Evaluation"
      ],
      "id": "p2EFyb3rKtVB"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EtaoYYGJ6J2"
      },
      "source": [
        "def predict_hatespeech_multitask(input, tokenizer, model): \r\n",
        "  model.eval()\r\n",
        "  encodings = tokenizer(input, return_tensors='pt', padding=True, truncation=True, max_length=128)\r\n",
        "  \r\n",
        "  (out1, out2, out3) = model(**encodings)\r\n",
        "  \r\n",
        "  preds_a = torch.max(out1, 1)\r\n",
        "  preds_b = torch.max(out2, 1)\r\n",
        "  preds_c = torch.max(out3, 1)\r\n",
        "\r\n",
        "  preds = (preds_a[1], preds_b[1], preds_c[1])\r\n",
        "  scores = (preds_a[0], preds_b[0], preds_c[0])\r\n",
        "\r\n",
        "  return {'predictions':preds, 'confidences':scores}"
      ],
      "id": "8EtaoYYGJ6J2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5pIqvWoLbDy"
      },
      "source": [
        "def evaluate_multitask(model, tokenizer, data_loader):\r\n",
        "\r\n",
        "  task_num = 3\r\n",
        "  total_count = 0\r\n",
        "  correct_count = [0] * task_num  \r\n",
        "  accuracies = [0] * task_num\r\n",
        "\r\n",
        "  batch_size = data_loader.batch_size\r\n",
        "\r\n",
        "  with torch.no_grad():\r\n",
        "    for data in tqdm(data_loader): \r\n",
        "\r\n",
        "      labels = {}\r\n",
        "      labels['label_a'] = data['label_a']\r\n",
        "      labels['label_b'] = data['label_b']\r\n",
        "      labels['label_c'] = data['label_c']\r\n",
        "\r\n",
        "      tweets = data['text']\r\n",
        "\r\n",
        "      pred = predict_hatespeech_multitask(tweets, tokenizer, model)\r\n",
        "\r\n",
        "      preds = pred['predictions'] \r\n",
        "\r\n",
        "      for i, label in enumerate(labels):\r\n",
        "        correct_count[i]+= torch.mean((preds[i] == labels[label]).float())\r\n",
        "\r\n",
        "      total_count += np.float(batch_size)\r\n",
        "\r\n",
        "    for i, label in enumerate(labels):\r\n",
        "      accuracies[i] = (correct_count[i]/total_count)\r\n",
        "\r\n",
        " \r\n",
        "  return accuracies"
      ],
      "id": "z5pIqvWoLbDy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P24QVnAULiMd"
      },
      "source": [
        "\r\n",
        "model = BERT_hate_speech_multitask.from_pretrained(\"./models/ht_bert_multi_finetuned/\")\r\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\r\n",
        "\r\n",
        "test_loader = DataLoader(test_dataset)\r\n",
        "\r\n",
        "accuracies = evaluate_multitask(model, tokenizer, test_loader)\r\n"
      ],
      "id": "P24QVnAULiMd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-h8pw9i8L8NJ"
      },
      "source": [
        "for i in range(3):\r\n",
        "    print('Task %d accuracy: %2.2f %%' % (i, 100.0*accuracies[i]))\r\n",
        "    "
      ],
      "id": "-h8pw9i8L8NJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA_vw43f-UX4"
      },
      "source": [
        "print(predict_hatespeech_multitask(\"I go see pinguins at the zoo.\", tokenizer, model)['predictions'])\r\n",
        "print(predict_hatespeech_multitask(\"Bananas are so stupid \", tokenizer, model)['predictions'])"
      ],
      "id": "yA_vw43f-UX4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "exterior-afghanistan"
      },
      "source": [
        "# Machine Translation (MT)"
      ],
      "id": "exterior-afghanistan"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "assigned-sapphire"
      },
      "source": [
        "Recall the previous lab on machine translation [(Lab 6)](https://colab.research.google.com/github/ImperialNLP/NLPLabs/blob/master/lab05/lab05_solutions.ipynb) where we trained a recurrent neural network on the [Multi30k](https://github.com/multi30k/dataset) dataset. \r\n",
        "\r\n",
        "We will now train a Transformer model for the same task on the same dataset and compare the results. "
      ],
      "id": "assigned-sapphire"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkbsnSm_oK8e"
      },
      "source": [
        "## Downloading dataset and evaluation function\r\n"
      ],
      "id": "wkbsnSm_oK8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fr-2a2z5kS9a"
      },
      "source": [
        "We will start by downloading the dataset for German, English and French and installing `sacreBLEU` to compute the BLEU score."
      ],
      "id": "fr-2a2z5kS9a"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "musical-series"
      },
      "source": [
        "%%bash\n",
        "\n",
        "# install sacreBLEU\n",
        "pip install sacrebleu==1.5.0\n",
        "echo\n",
        "\n",
        "# Download the corpus\n",
        "URL=\"https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/tok\"\n",
        "\n",
        "#cd data\n",
        "\n",
        "for split in \"train\" \"val\" \"test_2016_flickr\"; do\n",
        "    for lang in en de fr; do\n",
        "        fname=\"${split}.lc.norm.tok.${lang}\"\n",
        "        if [ ! -f $fname ]; then\n",
        "            echo \"Downloading $fname\"\n",
        "            wget -q \"${URL}/$fname\" -O \"${split/_2016_flickr/}.${lang}\"\n",
        "        fi\n",
        "    done\n",
        "done\n",
        "echo \n",
        "\n",
        "# Print the first 10 lines with line numbers of \n",
        "# the English and French training data\n",
        "cat -n train.en | head -n10\n",
        "echo\n",
        "cat -n train.fr | head -n10\n",
        "echo\n",
        "cd .."
      ],
      "id": "musical-series",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAB2AV_Ns-aY"
      },
      "source": [
        "## Dataset"
      ],
      "id": "qAB2AV_Ns-aY"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hs-Wy6uZkyJF"
      },
      "source": [
        "Just as in the previous lab we define our own dataset class to handle ``Multi30k``. However, we modify it to adapt it to a transformer model.\r\n",
        "\r\n"
      ],
      "id": "Hs-Wy6uZkyJF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wanted-venice"
      },
      "source": [
        "class Multi30K:\n",
        "    \"\"\"A dataset wrapper for Multi30K.\"\"\"\n",
        "    def __init__(self, tokenizer, src_file, trg_file):\n",
        "\n",
        "        self.tokenizer = tokenizer\n",
        "    \n",
        "        self.src_sents, self.trg_sents = self.read_sentences(src_file, trg_file)\n",
        "\n",
        "    def read_sentences(self, src_file, trg_file):\n",
        "        src_sents = []\n",
        "        trg_sents = []\n",
        "\n",
        "        # Read source side\n",
        "        with open(src_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                src_sents.append(line) \n",
        "            \n",
        "        # Read target side\n",
        "        with open(trg_file) as f:\n",
        "            for line in f:\n",
        "                line = line.strip()\n",
        "                trg_sents.append(line)\n",
        "\n",
        "        assert len(src_sents) == len(trg_sents), \"Files are not aligned!\"\n",
        "        return src_sents, trg_sents\n",
        "    \n",
        "    def collate_fn(self, idx):\n",
        "        src_texts = [self.src_sents[i] for i in idx]\n",
        "        trg_texts = [self.trg_sents[i] for i in idx]\n",
        "        \n",
        "        output = self.tokenizer.prepare_seq2seq_batch(src_texts=src_texts, \n",
        "                                                      tgt_texts=trg_texts, \n",
        "                                                      max_length=128, \n",
        "                                                      max_target_length=128,\n",
        "                                                      return_tensors='pt',\n",
        "                                                      truncation=True)\n",
        "        return output\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.src_sents)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return idx"
      ],
      "id": "wanted-venice",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xyGLGLBArnHL"
      },
      "source": [
        "## Transformer model for MT"
      ],
      "id": "xyGLGLBArnHL"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh7mzoLBmQhJ"
      },
      "source": [
        "\r\n",
        "For this task, we won't be using Bert but instead, we will import another model, [MarianMT](https://huggingface.co/transformers/model_doc/marian.html), which is a  specifically used for MT. \r\n",
        "\r\n",
        "Running the code below you can see all modules within MarianMT model and compare it with Bert. \r\n",
        "\r\n",
        "**Q4: What's missing in BERT architecture necessary for MT?**\r\n",
        "\r\n",
        "**ANS:** BERT doesn't have a decoder module. \r\n"
      ],
      "id": "Uh7mzoLBmQhJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SABFaL3UlK6r"
      },
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\r\n",
        "\r\n",
        "model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\r\n",
        "\r\n",
        "model"
      ],
      "id": "SABFaL3UlK6r",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "linear-diabetes"
      },
      "source": [
        "\n",
        "**Question 5: Import Transformer model and train for Machine Translation**\n",
        "\n",
        "Modify ``main_mt()`` to:\n",
        "- Import the ``\"Helsinki-NLP/opus-mt-en-de\"`` variant of ``MarianMT`` model and the tokenizer it was trained with.\n",
        "- Create the train (``train.en`` and ``train.de`` files) and test (``test.en`` and ``test.de``) dataset objects using our ``Multi30k`` class.\n",
        "- Finetune the model on the train set."
      ],
      "id": "linear-diabetes"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fewer-england"
      },
      "source": [
        "from transformers import EncoderDecoderModel, MarianMTModel, MarianTokenizer, BartModel, BartConfig, BertConfig, BartForCausalLM,Trainer,TrainingArguments\n",
        "\n",
        "def main_mt():\n",
        "    \n",
        "    ## QUESTION 5 ##\n",
        "\n",
        "    mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "    mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\n",
        "    mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\n",
        "    \n",
        "    model = MarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-de')\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir='./experiment/mt',\n",
        "        learning_rate = 0.00005,\n",
        "        logging_steps= 5000,\n",
        "        save_steps = 10000,\n",
        "        num_train_epochs = 1,\n",
        "        per_device_train_batch_size=2\n",
        "    )\n",
        "    trainer = Trainer(\n",
        "        model=model,                         \n",
        "        args=training_args,                 \n",
        "        train_dataset=mt_dataset,                     \n",
        "        data_collator=mt_dataset.collate_fn\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "\n",
        "    ## when you already trained your model and want to start from a checkpoint\n",
        "    #trainer.train(\"./experiment/mt/checkpoint-40000\")\n",
        "\n",
        "    trainer.save_model('./models/mt_marianmt/')\n"
      ],
      "id": "fewer-england",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8LdM1J-wtPWd"
      },
      "source": [
        "The following code should run for ~50min if you train the model for 3 epochs."
      ],
      "id": "8LdM1J-wtPWd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "resident-sight"
      },
      "source": [
        "main_mt()"
      ],
      "id": "resident-sight",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJcA9ihzY6Pf"
      },
      "source": [
        "Let's evaluate our model using BLEU metric\r\n",
        "."
      ],
      "id": "mJcA9ihzY6Pf"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rowUbyM_enUw"
      },
      "source": [
        "import sacrebleu\r\n",
        "\r\n",
        "def evaluate_mt(model,mt_tokenizer, mt_test_dataset):\r\n",
        "\r\n",
        "  bleu = []\r\n",
        "\r\n",
        "  model.eval()\r\n",
        "\r\n",
        "  for file in tqdm(range(len(mt_test_dataset))):\r\n",
        "\r\n",
        "    src_text = mt_test_dataset.src_sents[file]\r\n",
        "    targ_text_origin = mt_test_dataset.trg_sents[file]\r\n",
        "\r\n",
        "    translated = model.generate(**mt_tokenizer.prepare_seq2seq_batch(src_text, return_tensors=\"pt\"))\r\n",
        "    translated_text = [mt_tokenizer.decode(t, skip_special_tokens=True) for t in translated]\r\n",
        "\r\n",
        "    bleu.append(sacrebleu.corpus_bleu(translated_text, targ_text_origin, force=True).score)\r\n",
        "\r\n",
        "  bleu = np.asarray(bleu)\r\n",
        "\r\n",
        "  return np.average(bleu)"
      ],
      "id": "rowUbyM_enUw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OghdALVBpz1b"
      },
      "source": [
        "model = MarianMTModel.from_pretrained('./models/mt_marianmt/')\r\n",
        "\r\n",
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\r\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\r\n",
        "\r\n",
        "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\r\n",
        "\r\n",
        "print(bleu)\r\n"
      ],
      "id": "OghdALVBpz1b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfQXV7TABFVw"
      },
      "source": [
        "What BLEU score do you get ? How does it compare to the performance of the RNN with and without attention? \r\n",
        "\r\n",
        "Can you improve the score ?"
      ],
      "id": "dfQXV7TABFVw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zrLXxS1hHjuu"
      },
      "source": [
        "# Extra "
      ],
      "id": "zrLXxS1hHjuu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIuj99ZNHl8z"
      },
      "source": [
        "## Questions"
      ],
      "id": "aIuj99ZNHl8z"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MznOidHWnfD7"
      },
      "source": [
        "**Q6.1**: Why do RNN suffer from long term dependency issues and how do Transformers fix that?\r\n",
        "\r\n",
        "**ANS**: RNN based models process tokens one by one and each state depend on previous seen states. In vanilla RNN each token encoding relies on the previous step only. As the sentence grows in size, earlier tokens are forgotten (the gradient tends to vanish/explode after multiple passes through layers).\r\n",
        "LSTM and GRU architectures deal with some of the problem with the vanishing/exploding gradient and are able to handle longer sentences, but still can't handle very long inputs without forgetting earlier parts. \r\n",
        "\r\n",
        "Another shortcoming is that RNN process sentences in one direction and cannot look into future token, only the ones preceding the current state. Bi-directional models were introduced to solve that issue but add more computations as the model needs to go through the sequences twice. \r\n",
        "\r\n",
        "Thanks to self-attention and positional encodings, Transformers are able to compute relationships between words and their position without sequential training/inference. \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "\r\n",
        "**Q6.2**: Which architecture is faster to train: RNN or Transformer? Why?\r\n",
        "\r\n",
        "**ANS**: RNNs are complex stuctures due to the recursive training and because they take one token at a time, the training process is quite slow. They are also not parallelizable, since you can't compute hidden states of one word without first computing the representations of the previous words. Transformers, on the other hand, can process each step independently. \r\n",
        "\r\n",
        "---\r\n",
        "\r\n",
        "**Q6.3** What issues do Transformers have due to the attention mechanism? \r\n",
        "\r\n",
        "**ANS**: Self-attention becomes computationaly expensive as the input size increases. Because we compute attention for every pair of tokens, the time complexity is $O(N^2)$ with $N$ being the input length.\r\n",
        "On the other hand, RNNs are linear with the length of the input sequence  and have a computing complexity of $O(N)$. However, they still suffer from the issues mentioned above and remain slower to train.   \r\n",
        "\r\n",
        "However, Transformers can be modified to alleviate this issue (e.g transformer XL applies the simple truncated backpropagation, see lecture on Transformers).\r\n",
        "\r\n",
        "---"
      ],
      "id": "MznOidHWnfD7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPbsBF2LHpLV"
      },
      "source": [
        "## MT Extension"
      ],
      "id": "HPbsBF2LHpLV"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qREhnCecNhy"
      },
      "source": [
        "In this section, we mainly define the following classes:\r\n",
        "\r\n",
        "\r\n",
        "*   ``SinusoidalPositionalEmbedding`` -- *Note that this is different from the PositionalEmbedding in original Bert model.*\r\n",
        "*   ``Attention``\r\n",
        "*   ``PreTrainedModel``\r\n",
        "\r\n",
        "\r\n",
        "*   ``EncoderLayer``\r\n",
        "*   ``DecoderLayer``\r\n",
        "*   ``Encoder``\r\n",
        "*   ``Decoder``\r\n",
        "\r\n",
        "\r\n",
        "*   ``Model``\r\n",
        "*   ``MTModel``\r\n",
        "\r\n",
        "\r\n",
        "The task is to fill in the missing blocks in ***Model*** and ***MTModel*** and understand the process of transformer for MT. \r\n",
        "\r\n",
        "\r\n",
        "Search the coding blocks by ***TODO***"
      ],
      "id": "3qREhnCecNhy"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uub3GiD9QT8F"
      },
      "source": [
        "from transformers import PreTrainedModel, MarianConfig as MyConfig\n",
        "from transformers.activations import ACT2FN\n",
        "from transformers.modeling_outputs import (BaseModelOutput, \n",
        "    BaseModelOutputWithPastAndCrossAttentions,\n",
        "    CausalLMOutputWithCrossAttentions,\n",
        "    Seq2SeqLMOutput,\n",
        "    Seq2SeqModelOutput)\n",
        "\n",
        "import copy\n",
        "import math\n",
        "import random\n",
        "import logging\n",
        "from typing import Optional, Tuple\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.checkpoint\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "_PRETRAINED_MODEL_ARCHIVE_LIST = [\n",
        "    \"Helsinki-NLP/opus-mt-en-de\",\n",
        "]"
      ],
      "id": "Uub3GiD9QT8F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XuFULwNcSIkr"
      },
      "source": [
        "def shift_tokens_right(input_ids: torch.Tensor, pad_token_id: int, decoder_start_token_id: int):\n",
        "    \"\"\"\n",
        "    Shift input ids one token to the right.\n",
        "    \"\"\"\n",
        "    shifted_input_ids = input_ids.new_zeros(input_ids.shape)\n",
        "    shifted_input_ids[:, 1:] = input_ids[:, :-1].clone()\n",
        "    shifted_input_ids[:, 0] = decoder_start_token_id\n",
        "\n",
        "    assert pad_token_id is not None, \"self.model.config.pad_token_id has to be defined.\"\n",
        "    # replace possible -100 values in labels by `pad_token_id`\n",
        "    shifted_input_ids.masked_fill_(shifted_input_ids == -100, pad_token_id)\n",
        "\n",
        "    return shifted_input_ids\n",
        "\n",
        "\n",
        "\n",
        "def _make_causal_mask(input_ids_shape: torch.Size, dtype: torch.dtype, past_key_values_length: int = 0):\n",
        "    \"\"\"\n",
        "    Make causal mask used for bi-directional self-attention.\n",
        "    \"\"\"\n",
        "    bsz, tgt_len = input_ids_shape\n",
        "    mask = torch.full((tgt_len, tgt_len), float(\"-inf\"))\n",
        "    mask_cond = torch.arange(mask.size(-1))\n",
        "    mask.masked_fill_(mask_cond < (mask_cond + 1).view(mask.size(-1), 1), 0)\n",
        "    mask = mask.to(dtype)\n",
        "\n",
        "    if past_key_values_length > 0:\n",
        "        mask = torch.cat([torch.zeros(tgt_len, past_key_values_length, dtype=dtype), mask], dim=-1)\n",
        "    return mask[None, None, :, :].expand(bsz, 1, tgt_len, tgt_len + past_key_values_length)\n",
        "\n",
        "\n",
        "\n",
        "def _expand_mask(mask: torch.Tensor, dtype: torch.dtype, tgt_len: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Expands attention_mask from `[bsz, seq_len]` to `[bsz, 1, tgt_seq_len, src_seq_len]`.\n",
        "    \"\"\"\n",
        "    bsz, src_len = mask.size()\n",
        "    tgt_len = tgt_len if tgt_len is not None else src_len\n",
        "\n",
        "    expanded_mask = mask[:, None, None, :].expand(bsz, 1, tgt_len, src_len).to(dtype)\n",
        "\n",
        "    inverted_mask = 1.0 - expanded_mask\n",
        "\n",
        "    return inverted_mask.masked_fill(inverted_mask.bool(), torch.finfo(dtype).min)"
      ],
      "id": "XuFULwNcSIkr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5TyyQpsSKhg"
      },
      "source": [
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    \"\"\"This module produces sinusoidal positional embeddings of any length.\"\"\"\n",
        "\n",
        "    def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "\n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        \"\"\"\n",
        "        Identical to the XLM create_sinusoidal_embeddings except features are not interleaved. The cos features are in\n",
        "        the 2nd half of the vector. [dim // 2:]\n",
        "        \"\"\"\n",
        "        n_pos, dim = out.shape\n",
        "        position_enc = np.array(\n",
        "            [[pos / np.power(10000, 2 * (j // 2) / dim) for j in range(dim)] for pos in range(n_pos)]\n",
        "        )\n",
        "        out.requires_grad = False  # set early to avoid an error in pytorch-1.8+\n",
        "        sentinel = dim // 2 if dim % 2 == 0 else (dim // 2) + 1\n",
        "        out[:, 0:sentinel] = torch.FloatTensor(np.sin(position_enc[:, 0::2]))\n",
        "        out[:, sentinel:] = torch.FloatTensor(np.cos(position_enc[:, 1::2]))\n",
        "        out.detach_()\n",
        "        return out\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids_shape: torch.Size, past_key_values_length: int = 0):\n",
        "        \"\"\"`input_ids_shape` is expected to be [bsz x seqlen].\"\"\"\n",
        "        bsz, seq_len = input_ids_shape[:2]\n",
        "        positions = torch.arange(\n",
        "            past_key_values_length, past_key_values_length + seq_len, dtype=torch.long, device=self.weight.device\n",
        "        )\n",
        "        return super().forward(positions)"
      ],
      "id": "l5TyyQpsSKhg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eqqb50tESOpp"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embed_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout: float = 0.0,\n",
        "        is_decoder: bool = False,\n",
        "        bias: bool = True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.embed_dim = embed_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = embed_dim // num_heads\n",
        "        assert (\n",
        "            self.head_dim * num_heads == self.embed_dim\n",
        "        ), f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`: {num_heads}).\"\n",
        "        self.scaling = self.head_dim ** -0.5\n",
        "        self.is_decoder = is_decoder\n",
        "\n",
        "        self.k_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(embed_dim, embed_dim, bias=bias)\n",
        "\n",
        "    def _shape(self, tensor: torch.Tensor, seq_len: int, bsz: int):\n",
        "        return tensor.view(bsz, seq_len, self.num_heads, self.head_dim).transpose(1, 2).contiguous()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        key_value_states: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        output_attentions: bool = False,\n",
        "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
        "        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n",
        "\n",
        "        # if key_value_states are provided this layer is used as a cross-attention layer\n",
        "        # for the decoder\n",
        "        is_cross_attention = key_value_states is not None\n",
        "        bsz, tgt_len, embed_dim = hidden_states.size()\n",
        "\n",
        "        # get query proj\n",
        "        query_states = self.q_proj(hidden_states) * self.scaling\n",
        "        # get key, value proj\n",
        "        if is_cross_attention and past_key_value is not None:\n",
        "            # reuse k,v, cross_attentions\n",
        "            key_states = past_key_value[0]\n",
        "            value_states = past_key_value[1]\n",
        "        elif is_cross_attention:\n",
        "            # cross_attentions\n",
        "            key_states = self._shape(self.k_proj(key_value_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(key_value_states), -1, bsz)\n",
        "        elif past_key_value is not None:\n",
        "            # reuse k, v, self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "            key_states = torch.cat([past_key_value[0], key_states], dim=2)\n",
        "            value_states = torch.cat([past_key_value[1], value_states], dim=2)\n",
        "        else:\n",
        "            # self_attention\n",
        "            key_states = self._shape(self.k_proj(hidden_states), -1, bsz)\n",
        "            value_states = self._shape(self.v_proj(hidden_states), -1, bsz)\n",
        "\n",
        "        if self.is_decoder:\n",
        "            # if cross_attention save Tuple(torch.Tensor, torch.Tensor) of all cross attention key/value_states.\n",
        "            # Further calls to cross_attention layer can then reuse all cross-attention\n",
        "            # key/value_states (first \"if\" case)\n",
        "            # if uni-directional self-attention (decoder) save Tuple(torch.Tensor, torch.Tensor) of\n",
        "            # all previous decoder key/value_states. Further calls to uni-directional self-attention\n",
        "            # can concat previous decoder key/value_states to current projected key/value_states (third \"elif\" case)\n",
        "            # if encoder bi-directional self-attention `past_key_value` is always `None`\n",
        "            past_key_value = (key_states, value_states)\n",
        "\n",
        "        proj_shape = (bsz * self.num_heads, -1, self.head_dim)\n",
        "        query_states = self._shape(query_states, tgt_len, bsz).view(*proj_shape)\n",
        "        key_states = key_states.view(*proj_shape)\n",
        "        value_states = value_states.view(*proj_shape)\n",
        "\n",
        "        src_len = key_states.size(1)\n",
        "        attn_weights = torch.bmm(query_states, key_states.transpose(1, 2))\n",
        "\n",
        "        assert attn_weights.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            src_len,\n",
        "        ), f\"Attention weights should be of size {(bsz * self.num_heads, tgt_len, src_len)}, but is {attn_weights.size()}\"\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            assert attention_mask.size() == (\n",
        "                bsz,\n",
        "                1,\n",
        "                tgt_len,\n",
        "                src_len,\n",
        "            ), f\"Attention mask should be of size {(bsz, 1, tgt_len, src_len)}, but is {attention_mask.size()}\"\n",
        "            attn_weights = attn_weights.view(bsz, self.num_heads, tgt_len, src_len) + attention_mask\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
        "\n",
        "        if layer_head_mask is not None:\n",
        "            assert layer_head_mask.size() == (\n",
        "                self.num_heads,\n",
        "            ), f\"Head mask for a single layer should be of size {(self.num_heads,)}, but is {layer_head_mask.size()}\"\n",
        "            attn_weights = layer_head_mask.view(1, -1, 1, 1) * attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "\n",
        "        if output_attentions:\n",
        "            # this operation is a bit akward, but it's required to\n",
        "            # make sure that attn_weights keeps its gradient.\n",
        "            # In order to do so, attn_weights have to reshaped\n",
        "            # twice and have to be reused in the following\n",
        "            attn_weights_reshaped = attn_weights.view(bsz, self.num_heads, tgt_len, src_len)\n",
        "            attn_weights = attn_weights_reshaped.view(bsz * self.num_heads, tgt_len, src_len)\n",
        "        else:\n",
        "            attn_weights_reshaped = None\n",
        "\n",
        "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
        "\n",
        "        attn_output = torch.bmm(attn_probs, value_states)\n",
        "\n",
        "        assert attn_output.size() == (\n",
        "            bsz * self.num_heads,\n",
        "            tgt_len,\n",
        "            self.head_dim,\n",
        "        ), f\"`attn_output` should be of size {(bsz, self.num_heads, tgt_len, self.head_dim)}, but is {attn_output.size()}\"\n",
        "\n",
        "        attn_output = (\n",
        "            attn_output.view(bsz, self.num_heads, tgt_len, self.head_dim)\n",
        "            .transpose(1, 2)\n",
        "            .reshape(bsz, tgt_len, embed_dim)\n",
        "        )\n",
        "\n",
        "        attn_output = self.out_proj(attn_output)\n",
        "\n",
        "        return attn_output, attn_weights_reshaped, past_key_value"
      ],
      "id": "eqqb50tESOpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_HNBj_4SZgk"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.encoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "        )\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.encoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.encoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: torch.Tensor,\n",
        "        layer_head_mask: torch.Tensor,\n",
        "        output_attentions: bool = False,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "        hidden_states, attn_weights, _ = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        if torch.isinf(hidden_states).any() or torch.isnan(hidden_states).any():\n",
        "            clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n",
        "            hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (attn_weights,)\n",
        "\n",
        "        return outputs"
      ],
      "id": "-_HNBj_4SZgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J4xQ5kL6SesZ"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__()\n",
        "        self.embed_dim = config.d_model\n",
        "\n",
        "        self.self_attn = Attention(\n",
        "            embed_dim=self.embed_dim,\n",
        "            num_heads=config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = ACT2FN[config.activation_function]\n",
        "        self.activation_dropout = config.activation_dropout\n",
        "\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.encoder_attn = Attention(\n",
        "            self.embed_dim,\n",
        "            config.decoder_attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            is_decoder=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "        self.fc1 = nn.Linear(self.embed_dim, config.decoder_ffn_dim)\n",
        "        self.fc2 = nn.Linear(config.decoder_ffn_dim, self.embed_dim)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.embed_dim)\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        hidden_states: torch.Tensor,\n",
        "        attention_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_hidden_states: Optional[torch.Tensor] = None,\n",
        "        encoder_attention_mask: Optional[torch.Tensor] = None,\n",
        "        layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        encoder_layer_head_mask: Optional[torch.Tensor] = None,\n",
        "        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n",
        "        output_attentions: Optional[bool] = False,\n",
        "        use_cache: Optional[bool] = True,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            hidden_states (:obj:`torch.FloatTensor`): input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            attention_mask (:obj:`torch.FloatTensor`): attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor`): cross attention input to the layer of shape `(seq_len, batch, embed_dim)`\n",
        "            encoder_attention_mask (:obj:`torch.FloatTensor`): encoder attention mask of size\n",
        "                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n",
        "            layer_head_mask (:obj:`torch.FloatTensor`): mask for attention heads in a given layer of size\n",
        "                `(config.encoder_attention_heads,)`.\n",
        "            encoder_layer_head_mask (:obj:`torch.FloatTensor`): mask for encoder attention heads in a given layer of\n",
        "                size `(config.encoder_attention_heads,)`.\n",
        "            past_key_value (:obj:`Tuple(torch.FloatTensor)`): cached past key and value projection states\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "        \"\"\"\n",
        "        residual = hidden_states\n",
        "\n",
        "        # Self Attention\n",
        "        # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n",
        "        self_attn_past_key_value = past_key_value[:2] if past_key_value is not None else None\n",
        "        # add present self-attn cache to positions 1,2 of present_key_value tuple\n",
        "        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n",
        "            hidden_states=hidden_states,\n",
        "            past_key_value=self_attn_past_key_value,\n",
        "            attention_mask=attention_mask,\n",
        "            layer_head_mask=layer_head_mask,\n",
        "            output_attentions=output_attentions,\n",
        "        )\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.self_attn_layer_norm(hidden_states)\n",
        "\n",
        "        # Cross-Attention Block\n",
        "        cross_attn_present_key_value = None\n",
        "        cross_attn_weights = None\n",
        "        if encoder_hidden_states is not None:\n",
        "            residual = hidden_states\n",
        "\n",
        "            # cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\n",
        "            cross_attn_past_key_value = past_key_value[-2:] if past_key_value is not None else None\n",
        "            hidden_states, cross_attn_weights, cross_attn_present_key_value = self.encoder_attn(\n",
        "                hidden_states=hidden_states,\n",
        "                key_value_states=encoder_hidden_states,\n",
        "                attention_mask=encoder_attention_mask,\n",
        "                layer_head_mask=encoder_layer_head_mask,\n",
        "                past_key_value=cross_attn_past_key_value,\n",
        "                output_attentions=output_attentions,\n",
        "            )\n",
        "            hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "            hidden_states = residual + hidden_states\n",
        "            hidden_states = self.encoder_attn_layer_norm(hidden_states)\n",
        "\n",
        "            # add cross-attn to positions 3,4 of present_key_value tuple\n",
        "            present_key_value = present_key_value + cross_attn_present_key_value\n",
        "\n",
        "        # Fully Connected\n",
        "        residual = hidden_states\n",
        "        hidden_states = self.activation_fn(self.fc1(hidden_states))\n",
        "        hidden_states = F.dropout(hidden_states, p=self.activation_dropout, training=self.training)\n",
        "        hidden_states = self.fc2(hidden_states)\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "        hidden_states = residual + hidden_states\n",
        "        hidden_states = self.final_layer_norm(hidden_states)\n",
        "\n",
        "        outputs = (hidden_states,)\n",
        "\n",
        "        if output_attentions:\n",
        "            outputs += (self_attn_weights, cross_attn_weights)\n",
        "\n",
        "        if use_cache:\n",
        "            outputs += (present_key_value,)\n",
        "\n",
        "        return outputs"
      ],
      "id": "J4xQ5kL6SesZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxLkSqyuREvQ"
      },
      "source": [
        "class PreTrainedModel(PreTrainedModel):\n",
        "    config_class = MyConfig\n",
        "    base_model_prefix = \"model\"\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        std = self.config.init_std\n",
        "        if isinstance(module, nn.Linear):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                module.bias.data.zero_()\n",
        "        elif isinstance(module, SinusoidalPositionalEmbedding):\n",
        "            pass\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            module.weight.data.normal_(mean=0.0, std=std)\n",
        "            if module.padding_idx is not None:\n",
        "                module.weight.data[module.padding_idx].zero_()\n",
        "\n",
        "    @property\n",
        "    def dummy_inputs(self):\n",
        "        pad_token = self.config.pad_token_id\n",
        "        input_ids = torch.tensor([[0, 6, 10, 4, 2], [0, 8, 12, 2, pad_token]], device=self.device)\n",
        "        dummy_inputs = {\n",
        "            \"attention_mask\": input_ids.ne(pad_token),\n",
        "            \"input_ids\": input_ids,\n",
        "            \"decoder_input_ids\": input_ids,\n",
        "        }\n",
        "        return dummy_inputs"
      ],
      "id": "jxLkSqyuREvQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2CaNtVNpQUGP"
      },
      "source": [
        "class Encoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a\n",
        "    :class:`EncoderLayer`.\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.encoder_layerdrop\n",
        "\n",
        "        embed_dim = config.d_model\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(embed_dim) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, embed_dim, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            embed_dim,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        inputs_embeds=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        embed_pos = self.embed_positions(input_shape)\n",
        "\n",
        "        hidden_states = inputs_embeds + embed_pos\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # expand attention_mask\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            attention_mask = _expand_mask(attention_mask, inputs_embeds.dtype)\n",
        "\n",
        "        encoder_states = () if output_hidden_states else None\n",
        "        all_attentions = () if output_attentions else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, encoder_layer in enumerate(self.layers):\n",
        "            if output_hidden_states:\n",
        "                encoder_states = encoder_states + (hidden_states,)\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):  # skip the layer\n",
        "                layer_outputs = (None, None)\n",
        "            else:\n",
        "                if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                    def create_custom_forward(module):\n",
        "                        def custom_forward(*inputs):\n",
        "                            return module(*inputs, output_attentions)\n",
        "\n",
        "                        return custom_forward\n",
        "\n",
        "                    layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                        create_custom_forward(encoder_layer),\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        (head_mask[idx] if head_mask is not None else None),\n",
        "                    )\n",
        "                else:\n",
        "                    layer_outputs = encoder_layer(\n",
        "                        hidden_states,\n",
        "                        attention_mask,\n",
        "                        layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                        output_attentions=output_attentions,\n",
        "                    )\n",
        "\n",
        "                hidden_states = layer_outputs[0]\n",
        "\n",
        "            if output_attentions:\n",
        "                all_attentions = all_attentions + (layer_outputs[1],)\n",
        "\n",
        "        if output_hidden_states:\n",
        "            encoder_states = encoder_states + (hidden_states,)\n",
        "\n",
        "        if not return_dict:\n",
        "            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n",
        "        return BaseModelOutput(\n",
        "            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n",
        "        )"
      ],
      "id": "2CaNtVNpQUGP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dhaT7vtjSs0D"
      },
      "source": [
        "class Decoder(PreTrainedModel):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
        "\n",
        "    Args:\n",
        "        config: MyConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config: MyConfig, embed_tokens: Optional[nn.Embedding] = None):\n",
        "        super().__init__(config)\n",
        "        self.dropout = config.dropout\n",
        "        self.layerdrop = config.decoder_layerdrop\n",
        "        self.padding_idx = config.pad_token_id\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_scale = math.sqrt(config.d_model) if config.scale_embedding else 1.0\n",
        "\n",
        "        if embed_tokens is not None:\n",
        "            self.embed_tokens = embed_tokens\n",
        "        else:\n",
        "            self.embed_tokens = nn.Embedding(config.vocab_size, config.d_model, self.padding_idx)\n",
        "\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings,\n",
        "            config.d_model,\n",
        "            self.padding_idx,\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.embed_tokens\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.embed_tokens = value\n",
        "\n",
        "    # Copied from transformers.models.bart.modeling_bart.BartDecoder._prepare_decoder_attention_mask\n",
        "    def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n",
        "        # create causal mask\n",
        "        # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "        combined_attention_mask = None\n",
        "        if input_shape[-1] > 1:\n",
        "            combined_attention_mask = _make_causal_mask(\n",
        "                input_shape, inputs_embeds.dtype, past_key_values_length=past_key_values_length\n",
        "            ).to(self.device)\n",
        "\n",
        "        if attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            expanded_attn_mask = _expand_mask(attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "            combined_attention_mask = (\n",
        "                expanded_attn_mask if combined_attention_mask is None else expanded_attn_mask + combined_attention_mask\n",
        "            )\n",
        "\n",
        "        return combined_attention_mask\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        encoder_hidden_states=None,\n",
        "        encoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        encoder_head_mask=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        Args:\n",
        "            input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):\n",
        "                Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you\n",
        "                provide it.\n",
        "\n",
        "                Indices can be obtained using :class:`~transformers.MarianTokenizer`. See\n",
        "                :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__`\n",
        "                for details.\n",
        "\n",
        "                `What are input IDs? <../glossary.html#input-ids>`__\n",
        "            attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "                Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            encoder_hidden_states (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, encoder_sequence_length, hidden_size)`, `optional`):\n",
        "                Sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention\n",
        "                of the decoder.\n",
        "            encoder_attention_mask (:obj:`torch.LongTensor` of shape :obj:`(batch_size, encoder_sequence_length)`, `optional`):\n",
        "                Mask to avoid performing cross-attention on padding tokens indices of encoder input_ids. Mask values\n",
        "                selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 for tokens that are **not masked**,\n",
        "                - 0 for tokens that are **masked**.\n",
        "\n",
        "                `What are attention masks? <../glossary.html#attention-mask>`__\n",
        "            head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            encoder_head_mask (:obj:`torch.Tensor` of shape :obj:`(num_layers, num_heads)`, `optional`):\n",
        "                Mask to nullify selected heads of the attention modules in encoder to avoid performing cross-attention\n",
        "                on hidden heads. Mask values selected in ``[0, 1]``:\n",
        "\n",
        "                - 1 indicates the head is **not masked**,\n",
        "                - 0 indicates the heas is **masked**.\n",
        "\n",
        "            past_key_values (:obj:`Tuple[Tuple[torch.Tensor]]` of length :obj:`config.n_layers` with each tuple having 2 tuples each of which has 2 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):\n",
        "                Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up\n",
        "                decoding.\n",
        "\n",
        "                If :obj:`past_key_values` are used, the user can optionally input only the last\n",
        "                :obj:`decoder_input_ids` (those that don't have their past key value states given to this model) of\n",
        "                shape :obj:`(batch_size, 1)` instead of all :obj:`decoder_input_ids`` of shape :obj:`(batch_size,\n",
        "                sequence_length)`.\n",
        "            inputs_embeds (:obj:`torch.FloatTensor` of shape :obj:`(batch_size, sequence_length, hidden_size)`, `optional`):\n",
        "                Optionally, instead of passing :obj:`input_ids` you can choose to directly pass an embedded\n",
        "                representation. This is useful if you want more control over how to convert :obj:`input_ids` indices\n",
        "                into associated vectors than the model's internal embedding lookup matrix.\n",
        "            output_attentions (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under\n",
        "                returned tensors for more detail.\n",
        "            output_hidden_states (:obj:`bool`, `optional`):\n",
        "                Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors\n",
        "                for more detail.\n",
        "            return_dict (:obj:`bool`, `optional`):\n",
        "                Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.\n",
        "        \"\"\"\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        # retrieve input_ids and inputs_embeds\n",
        "        if input_ids is not None and inputs_embeds is not None:\n",
        "            raise ValueError(\"You cannot specify both decoder_input_ids and decoder_inputs_embeds at the same time\")\n",
        "        elif input_ids is not None:\n",
        "            input_shape = input_ids.size()\n",
        "            input_ids = input_ids.view(-1, input_shape[-1])\n",
        "        elif inputs_embeds is not None:\n",
        "            input_shape = inputs_embeds.size()[:-1]\n",
        "        else:\n",
        "            raise ValueError(\"You have to specify either decoder_input_ids or decoder_inputs_embeds\")\n",
        "\n",
        "        # past_key_values_length\n",
        "        past_key_values_length = past_key_values[0][0].shape[2] if past_key_values is not None else 0\n",
        "\n",
        "        if inputs_embeds is None:\n",
        "            inputs_embeds = self.embed_tokens(input_ids) * self.embed_scale\n",
        "\n",
        "        attention_mask = self._prepare_decoder_attention_mask(\n",
        "            attention_mask, input_shape, inputs_embeds, past_key_values_length\n",
        "        )\n",
        "\n",
        "        # expand encoder attention mask\n",
        "        if encoder_hidden_states is not None and encoder_attention_mask is not None:\n",
        "            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n",
        "            encoder_attention_mask = _expand_mask(encoder_attention_mask, inputs_embeds.dtype, tgt_len=input_shape[-1])\n",
        "\n",
        "        # embed positions\n",
        "        positions = self.embed_positions(input_shape, past_key_values_length)\n",
        "\n",
        "        hidden_states = inputs_embeds + positions\n",
        "\n",
        "        hidden_states = F.dropout(hidden_states, p=self.dropout, training=self.training)\n",
        "\n",
        "        # decoder layers\n",
        "        all_hidden_states = () if output_hidden_states else None\n",
        "        all_self_attns = () if output_attentions else None\n",
        "        all_cross_attentions = () if (output_attentions and encoder_hidden_states is not None) else None\n",
        "        next_decoder_cache = () if use_cache else None\n",
        "\n",
        "        # check if head_mask has a correct number of layers specified if desired\n",
        "        if head_mask is not None:\n",
        "            assert head_mask.size()[0] == (\n",
        "                len(self.layers)\n",
        "            ), f\"The head_mask should be specified for {len(self.layers)} layers, but it is for {head_mask.size()[0]}.\"\n",
        "        for idx, decoder_layer in enumerate(self.layers):\n",
        "            # add LayerDrop (see https://arxiv.org/abs/1909.11556 for description)\n",
        "            if output_hidden_states:\n",
        "                all_hidden_states += (hidden_states,)\n",
        "            dropout_probability = random.uniform(0, 1)\n",
        "            if self.training and (dropout_probability < self.layerdrop):\n",
        "                continue\n",
        "\n",
        "            past_key_value = past_key_values[idx] if past_key_values is not None else None\n",
        "\n",
        "            if getattr(self.config, \"gradient_checkpointing\", False) and self.training:\n",
        "\n",
        "                if use_cache:\n",
        "                    logger.warn(\n",
        "                        \"`use_cache=True` is incompatible with `config.gradient_checkpointing=True`. Setting \"\n",
        "                        \"`use_cache=False`...\"\n",
        "                    )\n",
        "                    use_cache = False\n",
        "\n",
        "                def create_custom_forward(module):\n",
        "                    def custom_forward(*inputs):\n",
        "                        # None for past_key_value\n",
        "                        return module(*inputs, output_attentions, use_cache)\n",
        "\n",
        "                    return custom_forward\n",
        "\n",
        "                layer_outputs = torch.utils.checkpoint.checkpoint(\n",
        "                    create_custom_forward(decoder_layer),\n",
        "                    hidden_states,\n",
        "                    attention_mask,\n",
        "                    encoder_hidden_states,\n",
        "                    encoder_attention_mask,\n",
        "                    head_mask[idx] if head_mask is not None else None,\n",
        "                    encoder_head_mask[idx] if encoder_head_mask is not None else None,\n",
        "                    None,\n",
        "                )\n",
        "            else:\n",
        "\n",
        "                layer_outputs = decoder_layer(\n",
        "                    hidden_states,\n",
        "                    attention_mask=attention_mask,\n",
        "                    encoder_hidden_states=encoder_hidden_states,\n",
        "                    encoder_attention_mask=encoder_attention_mask,\n",
        "                    layer_head_mask=(head_mask[idx] if head_mask is not None else None),\n",
        "                    encoder_layer_head_mask=(encoder_head_mask[idx] if encoder_head_mask is not None else None),\n",
        "                    past_key_value=past_key_value,\n",
        "                    output_attentions=output_attentions,\n",
        "                    use_cache=use_cache,\n",
        "                )\n",
        "            hidden_states = layer_outputs[0]\n",
        "\n",
        "            if use_cache:\n",
        "                next_decoder_cache += (layer_outputs[3 if output_attentions else 1],)\n",
        "\n",
        "            if output_attentions:\n",
        "                all_self_attns += (layer_outputs[1],)\n",
        "\n",
        "                if encoder_hidden_states is not None:\n",
        "                    all_cross_attentions += (layer_outputs[2],)\n",
        "\n",
        "        # add hidden states from the last decoder layer\n",
        "        if output_hidden_states:\n",
        "            all_hidden_states += (hidden_states,)\n",
        "\n",
        "        next_cache = next_decoder_cache if use_cache else None\n",
        "        if not return_dict:\n",
        "            return tuple(\n",
        "                v\n",
        "                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_cross_attentions]\n",
        "                if v is not None\n",
        "            )\n",
        "        return BaseModelOutputWithPastAndCrossAttentions(\n",
        "            last_hidden_state=hidden_states,\n",
        "            past_key_values=next_cache,\n",
        "            hidden_states=all_hidden_states,\n",
        "            attentions=all_self_attns,\n",
        "            cross_attentions=all_cross_attentions,\n",
        "        )"
      ],
      "id": "dhaT7vtjSs0D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMt1N5teXskj"
      },
      "source": [
        "There are 6 TODOs in the **Model**."
      ],
      "id": "YMt1N5teXskj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQM4v4HlS65I"
      },
      "source": [
        "class Model(PreTrainedModel):\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "        padding_idx, vocab_size = config.pad_token_id, config.vocab_size\n",
        "\n",
        "        ##  TODO 1: initialise embedding layer with nn.Embedding ##\n",
        "        self.shared = nn.Embedding(vocab_size, config.d_model, padding_idx)\n",
        "\n",
        "\n",
        "        ##  TODO 2: define the encoder and decoder with previously defined classes ##\n",
        "        self.encoder = Encoder(config, self.shared)\n",
        "        self.decoder = Decoder(config, self.shared)\n",
        "\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_input_embeddings(self):\n",
        "        return self.shared\n",
        "\n",
        "    def set_input_embeddings(self, value):\n",
        "        self.shared = value\n",
        "        self.encoder.embed_tokens = self.shared\n",
        "        self.decoder.embed_tokens = self.shared\n",
        "\n",
        "    def get_encoder(self):\n",
        "        ##  TODO 3: return the encoder ##\n",
        "        return self.encoder\n",
        "\n",
        "    def get_decoder(self):\n",
        "        ##  TODO 4: return the decoder ##\n",
        "        return self.decoder\n",
        "      \n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "\n",
        "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
        "        output_hidden_states = (\n",
        "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
        "        )\n",
        "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if encoder_outputs is None:\n",
        "\n",
        "            ##  TODO 5: get the encoder outputs ##\n",
        "            encoder_outputs = self.encoder(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                head_mask=head_mask,\n",
        "                inputs_embeds=inputs_embeds,\n",
        "                output_attentions=output_attentions,\n",
        "                output_hidden_states=output_hidden_states,\n",
        "                return_dict=return_dict,\n",
        "            )\n",
        "\n",
        "\n",
        "\n",
        "        # If the user passed a tuple for encoder_outputs, we wrap it in a BaseModelOutput when return_dict=True\n",
        "        elif return_dict and not isinstance(encoder_outputs, BaseModelOutput):\n",
        "            encoder_outputs = BaseModelOutput(\n",
        "                last_hidden_state=encoder_outputs[0],\n",
        "                hidden_states=encoder_outputs[1] if len(encoder_outputs) > 1 else None,\n",
        "                attentions=encoder_outputs[2] if len(encoder_outputs) > 2 else None,\n",
        "            )\n",
        "\n",
        "        ##  TODO 6: get the decoder outputs ##\n",
        "        # decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\n",
        "        decoder_outputs = self.decoder(\n",
        "            input_ids=decoder_input_ids,\n",
        "            attention_mask=decoder_attention_mask,\n",
        "            encoder_hidden_states=encoder_outputs[0],\n",
        "            encoder_attention_mask=attention_mask,\n",
        "            head_mask=decoder_head_mask,\n",
        "            encoder_head_mask=head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "        if not return_dict:\n",
        "            return decoder_outputs + encoder_outputs\n",
        "\n",
        "        return Seq2SeqModelOutput(\n",
        "            last_hidden_state=decoder_outputs.last_hidden_state,\n",
        "            past_key_values=decoder_outputs.past_key_values,\n",
        "            decoder_hidden_states=decoder_outputs.hidden_states,\n",
        "            decoder_attentions=decoder_outputs.attentions,\n",
        "            cross_attentions=decoder_outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=encoder_outputs.last_hidden_state,\n",
        "            encoder_hidden_states=encoder_outputs.hidden_states,\n",
        "            encoder_attentions=encoder_outputs.attentions,\n",
        "        )"
      ],
      "id": "nQM4v4HlS65I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7lSTY1TmV93Y"
      },
      "source": [
        "In the following **MTModel**, fill in the missing parts in ***__init__*** and ***forward*** function."
      ],
      "id": "7lSTY1TmV93Y"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwmR18i5TCTX"
      },
      "source": [
        "class MTModel(PreTrainedModel):\n",
        "    base_model_prefix = \"model\"\n",
        "    _keys_to_ignore_on_load_missing = [\n",
        "        r\"final_logits_bias\",\n",
        "        r\"encoder\\.version\",\n",
        "        r\"decoder\\.version\",\n",
        "        r\"lm_head\\.weight\",\n",
        "        r\"embed_positions\",\n",
        "    ]\n",
        "\n",
        "    _keys_to_ignore_on_save = [\n",
        "        \"model.encoder.embed_positions.weight\",\n",
        "        \"model.decoder.embed_positions.weight\",\n",
        "    ]\n",
        "\n",
        "    def __init__(self, config: MyConfig):\n",
        "        super().__init__(config)\n",
        "\n",
        "         ##  TODO 7: Define the model ##\n",
        "        self.model = \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.register_buffer(\"final_logits_bias\", torch.zeros((1, self.model.shared.num_embeddings)))\n",
        "\n",
        "        ##  TODO 8: Define the head ##\n",
        "        self.lm_head = \n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def get_encoder(self):\n",
        "\n",
        "        return self.model.get_encoder()\n",
        "\n",
        "    def get_decoder(self):\n",
        "        return self.model.get_decoder()\n",
        "\n",
        "    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:\n",
        "        new_embeddings = super().resize_token_embeddings(new_num_tokens)\n",
        "        self._resize_final_logits_bias(new_num_tokens)\n",
        "        return new_embeddings\n",
        "\n",
        "    def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n",
        "        old_num_tokens = self.final_logits_bias.shape[-1]\n",
        "        if new_num_tokens <= old_num_tokens:\n",
        "            new_bias = self.final_logits_bias[:, :new_num_tokens]\n",
        "        else:\n",
        "            extra_bias = torch.zeros((1, new_num_tokens - old_num_tokens), device=self.final_logits_bias.device)\n",
        "            new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n",
        "        self.register_buffer(\"final_logits_bias\", new_bias)\n",
        "\n",
        "    def get_output_embeddings(self):\n",
        "        return self.lm_head\n",
        "\n",
        "    def set_output_embeddings(self, new_embeddings):\n",
        "        self.lm_head = new_embeddings\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids=None,\n",
        "        attention_mask=None,\n",
        "        decoder_input_ids=None,\n",
        "        decoder_attention_mask=None,\n",
        "        head_mask=None,\n",
        "        decoder_head_mask=None,\n",
        "        encoder_outputs=None,\n",
        "        past_key_values=None,\n",
        "        inputs_embeds=None,\n",
        "        decoder_inputs_embeds=None,\n",
        "        labels=None,\n",
        "        use_cache=None,\n",
        "        output_attentions=None,\n",
        "        output_hidden_states=None,\n",
        "        return_dict=None,\n",
        "    ):\n",
        "        r\"\"\"\n",
        "        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n",
        "            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,\n",
        "            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored\n",
        "            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.\n",
        "\n",
        "        Returns:\n",
        "\n",
        "        \"\"\"\n",
        "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
        "\n",
        "        if labels is not None:\n",
        "            if decoder_input_ids is None:\n",
        "                decoder_input_ids = shift_tokens_right(\n",
        "                    labels, self.config.pad_token_id, self.config.decoder_start_token_id\n",
        "                )\n",
        "        \n",
        "\n",
        "        ##  TODO 9: get the model output using self.model ##\n",
        "        outputs = self.model(\n",
        "            input_ids,\n",
        "            attention_mask=attention_mask,\n",
        "            decoder_input_ids=decoder_input_ids,\n",
        "            encoder_outputs=encoder_outputs,\n",
        "            decoder_attention_mask=decoder_attention_mask,\n",
        "            head_mask=head_mask,\n",
        "            decoder_head_mask=decoder_head_mask,\n",
        "            past_key_values=past_key_values,\n",
        "            inputs_embeds=inputs_embeds,\n",
        "            decoder_inputs_embeds=decoder_inputs_embeds,\n",
        "            use_cache=use_cache,\n",
        "            output_attentions=output_attentions,\n",
        "            output_hidden_states=output_hidden_states,\n",
        "            return_dict=return_dict,\n",
        "        )\n",
        "\n",
        "\n",
        "        lm_logits = self.lm_head(outputs[0]) + self.final_logits_bias\n",
        "\n",
        "        masked_lm_loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            masked_lm_loss = loss_fct(lm_logits.view(-1, self.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        if not return_dict:\n",
        "            output = (lm_logits,) + outputs[1:]\n",
        "            return ((masked_lm_loss,) + output) if masked_lm_loss is not None else output\n",
        "\n",
        "        return Seq2SeqLMOutput(\n",
        "            loss=masked_lm_loss,\n",
        "            logits=lm_logits,\n",
        "            past_key_values=outputs.past_key_values,\n",
        "            decoder_hidden_states=outputs.decoder_hidden_states,\n",
        "            decoder_attentions=outputs.decoder_attentions,\n",
        "            cross_attentions=outputs.cross_attentions,\n",
        "            encoder_last_hidden_state=outputs.encoder_last_hidden_state,\n",
        "            encoder_hidden_states=outputs.encoder_hidden_states,\n",
        "            encoder_attentions=outputs.encoder_attentions,\n",
        "        )\n",
        "\n",
        "\n",
        "    def prepare_inputs_for_generation(\n",
        "        self,\n",
        "        decoder_input_ids,\n",
        "        past=None,\n",
        "        attention_mask=None,\n",
        "        head_mask=None,\n",
        "        use_cache=None,\n",
        "        encoder_outputs=None,\n",
        "        **kwargs\n",
        "    ):\n",
        "        # cut decoder_input_ids if past is used\n",
        "        if past is not None:\n",
        "            decoder_input_ids = decoder_input_ids[:, -1:]\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": None,  # encoder_outputs is defined. input_ids not needed\n",
        "            \"encoder_outputs\": encoder_outputs,\n",
        "            \"past_key_values\": past,\n",
        "            \"decoder_input_ids\": decoder_input_ids,\n",
        "            \"attention_mask\": attention_mask,\n",
        "            \"head_mask\": head_mask,\n",
        "            \"use_cache\": use_cache,  # change this to avoid caching (presumably for debugging)\n",
        "        }\n",
        "\n",
        "    def prepare_decoder_input_ids_from_labels(self, labels: torch.Tensor):\n",
        "        return shift_tokens_right(labels, self.config.pad_token_id, self.config.decoder_start_token_id)\n",
        "\n",
        "    def adjust_logits_during_generation(self, logits, cur_len, max_length):\n",
        "        logits[:, self.config.pad_token_id] = float(\"-inf\")  # never predict pad token.\n",
        "        return logits\n",
        "\n",
        "    @staticmethod\n",
        "    def _reorder_cache(past, beam_idx):\n",
        "        reordered_past = ()\n",
        "        for layer_past in past:\n",
        "            # cached cross_attention states don't have to be reordered -> they are always the same\n",
        "            reordered_past += (\n",
        "                tuple(past_state.index_select(0, beam_idx) for past_state in layer_past[:2]) + layer_past[2:],\n",
        "            )\n",
        "        return reordered_past"
      ],
      "id": "MwmR18i5TCTX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bdr-vbyeHjd"
      },
      "source": [
        "Let's try the model. "
      ],
      "id": "-bdr-vbyeHjd"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9V_0tmUkeGmv"
      },
      "source": [
        "mt_tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-de')\r\n",
        "mt_dataset = Multi30K(mt_tokenizer, 'train.en', 'train.de')\r\n",
        "mt_test_dataset = Multi30K(mt_tokenizer, 'test.en', 'test.de')\r\n",
        "\r\n",
        "\r\n",
        "model = MTModel.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\r\n",
        "\r\n",
        "bleu = evaluate_mt(model,mt_tokenizer, mt_test_dataset)\r\n",
        "\r\n",
        "print(bleu)"
      ],
      "id": "9V_0tmUkeGmv",
      "execution_count": null,
      "outputs": []
    }
  ]
}
